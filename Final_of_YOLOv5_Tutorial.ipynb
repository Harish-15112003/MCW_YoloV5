{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "1ShztVeu0YG-",
        "ZY2VXXXu74w5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harish-15112003/MCW_YoloV5/blob/main/Final_of_YOLOv5_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Clone GitHub [repository](https://github.com/ultralytics/yolov5), install [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) and check PyTorch and GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da8b0029-80c5-404b-fbcb-164f34455b44"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt comet_ml  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-398-g5cdad892 Python-3.11.11 torch-2.5.1+cu124 CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 40.3/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "metadata": {
        "id": "8O-0WihKBEXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# 1. Detect\n",
        "\n",
        "`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n",
        "\n",
        "```shell\n",
        "python detect.py --source 0  # webcam\n",
        "                          img.jpg  # image\n",
        "                          vid.mp4  # video\n",
        "                          screen  # screenshot\n",
        "                          path/  # directory\n",
        "                         'path/*.jpg'  # glob\n",
        "                         'https://youtu.be/LNwODJXcvt4'  # YouTube\n",
        "                         'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR9ZbuQCH7FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea7f70c-5a0d-4f9b-fec0-18b3c87b9999"
      },
      "source": [
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images\n",
        "# display.Image(filename='runs/detect/exp/zidane.jpg', width=600)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=data/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-398-g5cdad892 Python-3.11.11 torch-2.5.1+cu124 CPU\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 93.1MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "image 1/2 /content/yolov5/data/images/bus.jpg: 640x480 4 persons, 1 bus, 1168.2ms\n",
            "image 2/2 /content/yolov5/data/images/zidane.jpg: 384x640 2 persons, 2 ties, 574.8ms\n",
            "Speed: 3.0ms pre-process, 871.5ms inference, 31.1ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkAzDWJ7cWTr"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/127574988-6a558aa1-d268-44b9-bf6b-62d4c605cc72.jpg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eq1SMWl6Sfn"
      },
      "source": [
        "# 2. Validate\n",
        "Validate a model's accuracy on the [COCO](https://cocodataset.org/#home) dataset's `val` or `test` splits. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQPtK1QYVaD_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "f5c3f272-ac4f-49d5-fc82-dcc07191c22a"
      },
      "source": [
        "# Download COCO val\n",
        "torch.hub.download_url_to_file('https://github.com/ultralytics/assets/releases/download/v0.0.0/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n",
        "!unzip -q tmp.zip -d ../datasets && rm tmp.zip  # unzip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 308M/780M [00:02<00:03, 145MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-27b85124e597>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Download COCO val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_url_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://github.com/ultralytics/assets/releases/download/v0.0.0/coco2017val.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tmp.zip'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# download (780M - 5000 images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip -q tmp.zip -d ../datasets && rm tmp.zip  # unzip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[possibly-undefined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhash_prefix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                     \u001b[0msha256\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[possibly-undefined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X58w8JLpMnjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b6f45f-6f6c-4e08-ea19-8a5fef3729e6"
      },
      "source": [
        "# Validate YOLOv5s on COCO val\n",
        "!python val.py --weights yolov5s.pt --data coco.yaml --img 640 --half"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/coco.yaml, weights=['yolov5s.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 🚀 v7.0-398-g5cdad892 Python-3.11.11 torch-2.5.1+cu124 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco/val2017... 4952 images, 48 backgrounds, 0 corrupt: 100% 5000/5000 [00:02<00:00, 1716.51it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco/val2017.cache\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0% 0/157 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aimet-torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W8YK0i4pJZBI",
        "outputId": "b9d11f7d-a532-4717-f803-d39b995849a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aimet-torch\n",
            "  Downloading aimet_torch-2.0.0-py38-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (3.6.3)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (1.6.2)\n",
            "Collecting dataclasses (from aimet-torch)\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (3.12.1)\n",
            "Requirement already satisfied: holoviews in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (1.20.1)\n",
            "Collecting hvplot (from aimet-torch)\n",
            "  Downloading hvplot-0.11.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: Jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (3.1.5)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (4.23.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (3.10.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (3.4.2)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (1.26.4)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (1.17.0)\n",
            "Collecting onnxscript (from aimet-torch)\n",
            "  Downloading onnxscript-0.2.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: osqp in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (0.6.7.post3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (2.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (5.9.5)\n",
            "Collecting pybind11 (from aimet-torch)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (0.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (1.13.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (75.1.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (2.18.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from aimet-torch) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.0.3->aimet-torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->aimet-torch) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->aimet-torch) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->aimet-torch) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->aimet-torch) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->aimet-torch) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->aimet-torch) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->aimet-torch) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->aimet-torch) (2.8.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh->aimet-torch) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh->aimet-torch) (2025.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->aimet-torch) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->aimet-torch) (2025.1)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy->aimet-torch) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy->aimet-torch) (3.2.7.post2)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.11/dist-packages (from osqp->aimet-torch) (0.1.7.post5)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.11/dist-packages (from holoviews->aimet-torch) (3.1.0)\n",
            "Requirement already satisfied: panel>=1.0 in /usr/local/lib/python3.11/dist-packages (from holoviews->aimet-torch) (1.6.1)\n",
            "Requirement already satisfied: param<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from holoviews->aimet-torch) (2.2.0)\n",
            "Requirement already satisfied: pyviz-comms>=2.1 in /usr/local/lib/python3.11/dist-packages (from holoviews->aimet-torch) (3.0.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->aimet-torch) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->aimet-torch) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->aimet-torch) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->aimet-torch) (0.23.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx->aimet-torch) (4.25.6)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from onnxscript->aimet-torch) (4.12.2)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.11/dist-packages (from onnxscript->aimet-torch) (0.4.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->aimet-torch) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->aimet-torch) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->aimet-torch) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->aimet-torch) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->aimet-torch) (3.7)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->aimet-torch) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->aimet-torch) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->aimet-torch) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (3.17.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->aimet-torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->aimet-torch) (1.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->aimet-torch) (6.2.0)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->aimet-torch) (2.0.3)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->aimet-torch) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->aimet-torch) (0.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->aimet-torch) (2.32.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->panel>=1.0->holoviews->aimet-torch) (0.5.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py->panel>=1.0->holoviews->aimet-torch) (1.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py->panel>=1.0->holoviews->aimet-torch) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=1.0->holoviews->aimet-torch) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=1.0->holoviews->aimet-torch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=1.0->holoviews->aimet-torch) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=1.0->holoviews->aimet-torch) (2025.1.31)\n",
            "Downloading aimet_torch-2.0.0-py38-none-any.whl (15.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Downloading hvplot-0.11.2-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.9/161.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxscript-0.2.2-py3-none-any.whl (694 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m694.9/694.9 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dataclasses, pybind11, onnxscript, hvplot, aimet-torch\n",
            "Successfully installed aimet-torch-2.0.0 dataclasses-0.6 hvplot-0.11.2 onnxscript-0.2.2 pybind11-2.13.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses"
                ]
              },
              "id": "77ac1a9b569843f5bc9d58533b5ecbe3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ovhM1Z7jLrGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Yolo inference and Aimet**"
      ],
      "metadata": {
        "id": "BUOlQC-ILsma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "class COCODataset(Dataset):\n",
        "    def __init__(self, root_dir, annotation_file, img_size=640, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Path to the COCO images directory.\n",
        "            annotation_file (str): Path to the COCO annotation file (JSON).\n",
        "            img_size (int): Target image size for YOLOv5.\n",
        "            transform: Torchvision transforms.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.coco = COCO(annotation_file)\n",
        "        self.img_ids = list(self.coco.imgs.keys())  # Get all image IDs\n",
        "        self.img_size = img_size\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.img_ids[index]\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        # Load image\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.root_dir, img_info[\"file_name\"])\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "        orig_h, orig_w = image.shape[:2]\n",
        "\n",
        "        # Resize to YOLO format\n",
        "        image = cv2.resize(image, (self.img_size, self.img_size))\n",
        "        image = image / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "        # Convert annotations to YOLO format\n",
        "        targets = []\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann[\"bbox\"]\n",
        "            class_id = ann[\"category_id\"]\n",
        "\n",
        "            # Convert to YOLO format (normalized)\n",
        "            x_center = (x + w / 2) / orig_w\n",
        "            y_center = (y + h / 2) / orig_h\n",
        "            w = w / orig_w\n",
        "            h = h / orig_h\n",
        "\n",
        "            targets.append([class_id, x_center, y_center, w, h])\n",
        "\n",
        "        targets = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return torch.tensor(image, dtype=torch.float32), targets\n",
        "\n"
      ],
      "metadata": {
        "id": "g_wY-uWmLrCe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = COCODataset(root_dir= \"/content/datasets/coco/images/val2017\", annotation_file= \"/content/datasets/coco/annotations/instances_val2017.json\", )\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwEsBkbVLq_B",
        "outputId": "f2be2f17-675d-41aa-9417-5fd32a5a118e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.08s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.COCODataset at 0x7eeeea4b29d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.__len__()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz_jzpmaLq8p",
        "outputId": "039ab246-46f5-44cb-a19d-1b22ce124601"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "   images, targets = zip(*batch)\n",
        "   images = torch.stack(images, dim=0)\n",
        "   return images , targets\n"
      ],
      "metadata": {
        "id": "mODLa_snQHJH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset=dataset ,  batch_size= 64 , num_workers=4  ,drop_last= True , collate_fn= collate_fn)"
      ],
      "metadata": {
        "id": "A1C1qJbbLq59"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj7VfmXQLq3h",
        "outputId": "cb2494d6-e5b1-4c28-e0ca-ba16f9ba1ccd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else  \"cpu\" )\n",
        "print(device)\n",
        "dummy_input = torch.rand(1 , 3, 640 , 640 ).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrbA4vbJLqyk",
        "outputId": "d8d47ae9-ef68-4b9b-a972-b74e6d623384"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path=\"/content/yolov5/yolov5s.pt\", force_reload=True)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9_sUSq1UIJq",
        "outputId": "8569961a-8e58-4dd3-dbdf-52120c9a54e9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 v7.0-398-g5cdad892 Python-3.11.11 torch-2.5.1+cu124 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoShape(\n",
              "  (model): DetectMultiBackend(\n",
              "    (model): DetectionModel(\n",
              "      (model): Sequential(\n",
              "        (0): Conv(\n",
              "          (conv): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv(\n",
              "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (2): C3(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv(\n",
              "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv3): Conv(\n",
              "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): Conv(\n",
              "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (4): C3(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv(\n",
              "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv3): Conv(\n",
              "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "            (1): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (5): Conv(\n",
              "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (6): C3(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv(\n",
              "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv3): Conv(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "            (1): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "            (2): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (7): Conv(\n",
              "          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (8): C3(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv(\n",
              "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv3): Conv(\n",
              "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (9): SPPF(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv(\n",
              "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "        (10): Conv(\n",
              "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (11): Upsample(scale_factor=2.0, mode='nearest')\n",
              "        (12): Concat()\n",
              "        (13): C3(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv(\n",
              "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv3): Conv(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (14): Conv(\n",
              "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (15): Upsample(scale_factor=2.0, mode='nearest')\n",
              "        (16): Concat()\n",
              "        (17): C3(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv(\n",
              "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv3): Conv(\n",
              "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (18): Conv(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (19): Concat()\n",
              "        (20): C3(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv(\n",
              "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv3): Conv(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (21): Conv(\n",
              "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (22): Concat()\n",
              "        (23): C3(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv(\n",
              "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv3): Conv(\n",
              "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (24): Detect(\n",
              "          (m): ModuleList(\n",
              "            (0): Conv2d(128, 255, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (2): Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aimet Guidelines"
      ],
      "metadata": {
        "id": "6pMUD3NyVUix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Export to onnx"
      ],
      "metadata": {
        "id": "4gIE1kr7VaAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoQvBLoFU7SF",
        "outputId": "9a52b22f-469f-4565-e379-9bec172aaf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[autoreload of models.yolo failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
            "    module = reload(module)\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/imp.py\", line 315, in reload\n",
            "    return importlib.reload(module)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 169, in reload\n",
            "    _bootstrap._exec(spec, module)\n",
            "  File \"<frozen importlib._bootstrap>\", line 621, in _exec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1074, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1004, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/content/yolov5/models/yolo.py\", line 104\n",
            "    if isinstance(self, Segment):  # (boxes + masks)\n",
            "                                                    ^\n",
            "IndentationError: unindent does not match any outer indentation level\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.25.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define dummy input matching the model's input shape\n",
        "dummy_input = torch.randn(1, 3, 640, 640)\n",
        "\n",
        "# Specify the output file name\n",
        "onnx_file_path = \"/content/model.onnx\"\n",
        "\n",
        "# Export the model\n",
        "torch.onnx.export(model, dummy_input, onnx_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH-yaYfiVTEX",
        "outputId": "16805c25-534f-4742-b891-0bbbfe0d2774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/yolov5/models/common.py:867: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:688: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Validator"
      ],
      "metadata": {
        "id": "9jsjKZFe-9-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare and validate model\n",
        "\n",
        "from aimet_torch.model_validator.model_validator import ModelValidator\n",
        "\n",
        "validate = ModelValidator.validate_model(model=model , model_input= dummy_input)\n",
        "\n",
        "if not validate:\n",
        "  print(\"Error\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO-t8K8CLqto",
        "outputId": "29ab7eee-63b3-483a-90c2-71a0f9a45e52"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-09 16:12:39,771 - Utils - INFO - Running validator check <function validate_for_reused_modules at 0x7eeef0a176a0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-09 16:12:40,318 - Utils - ERROR - The following modules are used more than once in the model: ['model.model.model.9.m']\n",
            "AIMET features are not designed to work with reused modules. Please redefine your model to use distinct modules for each instance.\n",
            "2025-03-09 16:12:40,320 - Utils - INFO - Running validator check <function validate_for_missing_modules at 0x7eeef0a17740>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-09 16:12:42,986 - Utils - ERROR - Functional ops that are not marked as math invariant were found in the model. AIMET features will not work properly for such ops.\n",
            "Consider the following choices: \n",
            "1. Redefine as a torch.nn.Module in the class definition.\n",
            "2. The op can remain as a functional op due to being math invariant, but the op type has not been added to ConnectedGraph.math_invariant_types set. \n",
            "Add an entry to ignore the op by importing the set and adding the op type:\n",
            "\n",
            "\tfrom aimet_torch.meta.connectedgraph import ConnectedGraph\n",
            "\tConnectedGraph.math_invariant_types.add(...)\n",
            "\n",
            "The following functional ops were found. The parent module is named for ease of locating the ops within the model definition.\n",
            "\ttype_as_9; op_type: type_as                              parent module: AutoShape\n",
            "\tAdd_21; op_type: Add                                     parent module: AutoShape.model.model.model.2.m.0\n",
            "\tConcat_26; op_type: Concat                               parent module: AutoShape.model.model.model.2\n",
            "\tAdd_38; op_type: Add                                     parent module: AutoShape.model.model.model.4.m.0\n",
            "\tAdd_44; op_type: Add                                     parent module: AutoShape.model.model.model.4.m.1\n",
            "\tConcat_49; op_type: Concat                               parent module: AutoShape.model.model.model.4\n",
            "\tAdd_61; op_type: Add                                     parent module: AutoShape.model.model.model.6.m.0\n",
            "\tAdd_67; op_type: Add                                     parent module: AutoShape.model.model.model.6.m.1\n",
            "\tAdd_73; op_type: Add                                     parent module: AutoShape.model.model.model.6.m.2\n",
            "\tConcat_78; op_type: Concat                               parent module: AutoShape.model.model.model.6\n",
            "\tAdd_90; op_type: Add                                     parent module: AutoShape.model.model.model.8.m.0\n",
            "\tConcat_95; op_type: Concat                               parent module: AutoShape.model.model.model.8\n",
            "\tConcat_105; op_type: Concat                              parent module: AutoShape.model.model.model.9\n",
            "\tConcat_122; op_type: Concat                              parent module: AutoShape.model.model.model.13\n",
            "\tConcat_139; op_type: Concat                              parent module: AutoShape.model.model.model.17\n",
            "\tConcat_155; op_type: Concat                              parent module: AutoShape.model.model.model.20\n",
            "\tConcat_171; op_type: Concat                              parent module: AutoShape.model.model.model.23\n",
            "\tarange_212; op_type: arange                              parent module: AutoShape.model.model.model.24\n",
            "\tarange_217; op_type: arange                              parent module: AutoShape.model.model.model.24\n",
            "\tmeshgrid_220; op_type: meshgrid                          parent module: AutoShape.model.model.model.24\n",
            "\tstack_224; op_type: stack                                parent module: AutoShape.model.model.model.24\n",
            "\tsub_233; op_type: sub                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_241; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tsigmoid_255; op_type: sigmoid                            parent module: AutoShape.model.model.model.24\n",
            "\tsplit_with_sizes_261; op_type: split_with_sizes          parent module: AutoShape.model.model.model.24\n",
            "\tMul_264; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tAdd_266; op_type: Add                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_270; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_272; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tpow_274; op_type: pow                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_275; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tConcat_278; op_type: Concat                              parent module: AutoShape.model.model.model.24\n",
            "\tMul_280; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_281; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tarange_323; op_type: arange                              parent module: AutoShape.model.model.model.24\n",
            "\tarange_328; op_type: arange                              parent module: AutoShape.model.model.model.24\n",
            "\tmeshgrid_331; op_type: meshgrid                          parent module: AutoShape.model.model.model.24\n",
            "\tstack_335; op_type: stack                                parent module: AutoShape.model.model.model.24\n",
            "\tsub_344; op_type: sub                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_351; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tsigmoid_365; op_type: sigmoid                            parent module: AutoShape.model.model.model.24\n",
            "\tsplit_with_sizes_371; op_type: split_with_sizes          parent module: AutoShape.model.model.model.24\n",
            "\tMul_374; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tAdd_376; op_type: Add                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_380; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_382; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tpow_384; op_type: pow                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_385; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tConcat_388; op_type: Concat                              parent module: AutoShape.model.model.model.24\n",
            "\tMul_390; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_391; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tarange_433; op_type: arange                              parent module: AutoShape.model.model.model.24\n",
            "\tarange_438; op_type: arange                              parent module: AutoShape.model.model.model.24\n",
            "\tmeshgrid_441; op_type: meshgrid                          parent module: AutoShape.model.model.model.24\n",
            "\tstack_445; op_type: stack                                parent module: AutoShape.model.model.model.24\n",
            "\tsub_454; op_type: sub                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_461; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tsigmoid_475; op_type: sigmoid                            parent module: AutoShape.model.model.model.24\n",
            "\tsplit_with_sizes_481; op_type: split_with_sizes          parent module: AutoShape.model.model.model.24\n",
            "\tMul_484; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tAdd_486; op_type: Add                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_490; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_492; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tpow_494; op_type: pow                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_495; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tConcat_498; op_type: Concat                              parent module: AutoShape.model.model.model.24\n",
            "\tMul_500; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tMul_501; op_type: Mul                                    parent module: AutoShape.model.model.model.24\n",
            "\tConcat_508; op_type: Concat                              parent module: AutoShape.model.model.model.24\n",
            "\n",
            "2025-03-09 16:12:42,987 - Utils - INFO - The following validator checks failed:\n",
            "2025-03-09 16:12:42,990 - Utils - INFO - \t<function validate_for_reused_modules at 0x7eeef0a176a0>\n",
            "2025-03-09 16:12:42,991 - Utils - INFO - \t<function validate_for_missing_modules at 0x7eeef0a17740>\n",
            "Error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aimet_torch.meta.connectedgraph import ConnectedGraph\n",
        "operations =[\"Mul\" , \"Concat\" , \"pow\" , \"sigmoid\" , \"split_with_sizes\", \"type_as\", \"Add\"]\n",
        "ConnectedGraph.math_invariant_types.update(operations)"
      ],
      "metadata": {
        "id": "wIW-Ds-jKkVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Model"
      ],
      "metadata": {
        "id": "1E3Zw3d0_Cgi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vfv7Dve8Z46y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from aimet_torch.model_preparer import prepare_model\n",
        "\n",
        "from aimet_torch.model_preparer import prepare_model\n",
        "\n",
        "# Exclude the entire model type to avoid tracing issues\n",
        "modules_to_exclude = [type(model)]  # Exclude whole model class\n",
        "concrete_args = {'size': (640, 640)}  # Provide a fixed input size\n",
        "\n",
        "# Prepare the model with exclusions and concrete arguments\n",
        "model_prepared = prepare_model(model=model, modules_to_exclude=modules_to_exclude, concrete_args=concrete_args)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "C16AyE55Lqqr",
        "outputId": "a2fdb2e3-add3-4ec2-e21a-312f9cdd3306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TraceError",
          "evalue": "symbolically traced variables cannot be used as inputs to control flow",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTraceError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-6116b1c03899>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Prepare the model with exclusions and concrete arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel_prepared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules_to_exclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodules_to_exclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcrete_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/model_preparer.py\u001b[0m in \u001b[0;36mprepare_model\u001b[0;34m(model, modules_to_exclude, module_classes_to_exclude, concrete_args)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0min_eval_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mtraced_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_name_to_scope\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0m_trace_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules_to_exclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_classes_to_exclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;31m# Prepare model and perform checks to make sure the graph is well-formed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/model_preparer.py\u001b[0m in \u001b[0;36m_trace_model\u001b[0;34m(model, modules_to_exclude, module_classes_to_exclude, concrete_args)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;31m# Symbolic tracing frontend - captures the semantics of the module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcrete_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0mtraced_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraced_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_name_to_scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    821\u001b[0m                     \u001b[0;34m\"output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;34m\"output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m                     \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m                     \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m                     \u001b[0mtype_expr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__annotations__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"return\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36mflatten_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mflatten_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0mtree_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                 \u001b[0mtree_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtree_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m                 \u001b[0mout_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_codegen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_PyTreeCodeGen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/yolov5/models/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[1;32m    881\u001b[0m                     \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexif_transpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"filename\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# image in CHW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m                     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# reverse dataloader .transpose(2, 0, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_GRAY2BGR\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# enforce 3ch input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/proxy.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    485\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcompatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_backward_compatible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/proxy.py\u001b[0m in \u001b[0;36mto_bool\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0minformation\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mnode\u001b[0m \u001b[0musing\u001b[0m \u001b[0mcreate_node\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mchoose\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \"\"\"\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTraceError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'symbolically traced variables cannot be used as inputs to control flow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcompatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_backward_compatible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTraceError\u001b[0m: symbolically traced variables cannot be used as inputs to control flow"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practise"
      ],
      "metadata": {
        "id": "naCQQpqlLX__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_collate_fn(batch):\n",
        "    images, targets = zip(*batch)  # Unzip images and targets\n",
        "\n",
        "    # Stack images into a single tensor (batch_size, C, H, W)\n",
        "    images = torch.stack(images)  # Ensure batch dimension is included\n",
        "\n",
        "    # Convert list of targets to tensor (cannot stack directly due to variable sizes)\n",
        "    return images, list(targets)"
      ],
      "metadata": {
        "id": "baqinmvvl0lT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torchvision.transforms as transforms\n",
        "# import torchvision.datasets as datasets\n",
        "# from torch.utils.data import DataLoader, Dataset\n",
        "# from pycocotools.coco import COCO\n",
        "# import os\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "\n",
        "# class COCODataset(Dataset):\n",
        "#     def __init__(self, root_dir, annotation_file, img_size=640, transform=None):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             root_dir (str): Path to the COCO images directory.\n",
        "#             annotation_file (str): Path to the COCO annotation file (JSON).\n",
        "#             img_size (int): Target image size for YOLOv5.\n",
        "#             transform: Torchvision transforms.\n",
        "#         \"\"\"\n",
        "#         self.root_dir = root_dir\n",
        "#         self.coco = COCO(annotation_file)\n",
        "#         self.img_ids = list(self.coco.imgs.keys())  # Get all image IDs\n",
        "#         self.img_size = img_size\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.img_ids)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         img_id = self.img_ids[index]\n",
        "#         ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "#         anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "#         # Load image\n",
        "#         img_info = self.coco.loadImgs(img_id)[0]\n",
        "#         img_path = os.path.join(self.root_dir, img_info[\"file_name\"])\n",
        "#         image = cv2.imread(img_path)\n",
        "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "#         orig_h, orig_w = image.shape[:2]\n",
        "\n",
        "#         # Resize to YOLO format\n",
        "#         image = cv2.resize(image, (self.img_size, self.img_size))\n",
        "#         image = image / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "#         # Convert annotations to YOLO format\n",
        "#         targets = []\n",
        "#         for ann in anns:\n",
        "#             x, y, w, h = ann[\"bbox\"]\n",
        "#             class_id = ann[\"category_id\"]\n",
        "\n",
        "#             # Convert to YOLO format (normalized)\n",
        "#             x_center = (x + w / 2) / orig_w\n",
        "#             y_center = (y + h / 2) / orig_h\n",
        "#             w = w / orig_w\n",
        "#             h = h / orig_h\n",
        "\n",
        "#             targets.append([class_id, x_center, y_center, w, h])\n",
        "\n",
        "#         targets = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "#         # Apply transforms\n",
        "#         if self.transform:\n",
        "#             image = self.transform(image)\n",
        "\n",
        "#         return torch.tensor(image, dtype=torch.float32), targets\n",
        "\n"
      ],
      "metadata": {
        "id": "SIb-x1PQuAxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from aimet_torch.v2.nn import QuantizationMixin\n",
        "from models.common import Concat\n",
        "\n",
        "@QuantizationMixin.implements(Concat)\n",
        "class QuantizedConcat(QuantizationMixin, Concat):\n",
        "    def __init__(self, dimension=1):\n",
        "        super().__init__(dimension)  # Pass 'dimension' to the parent Concat class\n",
        "        self.input_quantizers = torch.nn.ModuleList([None, None])\n",
        "        self.output_quantizers = torch.nn.ModuleList([None])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Quantize inputs\n",
        "        if self.input_quantizers[0]:\n",
        "            x = [self.input_quantizers[i](x[i]) for i in range(len(x))]\n",
        "\n",
        "        # Run forward with quantized inputs and parameters\n",
        "        with self._patch_quantized_parameters():\n",
        "            ret = super().forward(x)  # Calls Concat's forward()\n",
        "\n",
        "        # Quantize output tensors\n",
        "        if self.output_quantizers[0]:\n",
        "            ret = self.output_quantizers[0](ret)\n",
        "\n",
        "        return ret\n"
      ],
      "metadata": {
        "id": "Lanfa0ggFihd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import aimet_torch.quantsim as qsim\n",
        "from datetime import datetime\n",
        "import os\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from aimet_torch.batch_norm_fold import fold_all_batch_norms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "def forward_pass(model, dataloader, device=\"cpu\", num_batches=10):\n",
        "        \"\"\"\n",
        "        Performs a forward pass using a subset of data for quantization calibration.\n",
        "        Uses model.eval() and disables gradient computation for efficiency.\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (images, targets) in enumerate(dataloader):\n",
        "                images = images.to(device)  # Move images to GPU/CPU\n",
        "\n",
        "                # Ensure targets are properly handled (optional, if needed)\n",
        "                # targets = [t.to(device) for t in targets]  # Uncomment if necessary\n",
        "\n",
        "                model(images)  # Forward pass for calibration\n",
        "\n",
        "                if i >= num_batches - 1:  # Limit number of batches\n",
        "                    break\n",
        "\n",
        "\n",
        "def apply_aimet_quantization(model, dataloader, device='cpu'):\n",
        "    model.to(device)\n",
        "    dummy_input = torch.rand(1, 3, 640, 640).to(device)\n",
        "\n",
        "    # Step 1: Print model before BatchNorm folding\n",
        "    print(\"\\nModel before BatchNorm folding:\")\n",
        "    print(model)\n",
        "\n",
        "    # Step 2: Fold BatchNorm layers\n",
        "    fold_all_batch_norms(model, input_shapes=(1, 3, 640, 640), dummy_input=dummy_input)\n",
        "\n",
        "    # Step 3: Print model after BatchNorm folding\n",
        "    print(\"\\nModel after BatchNorm folding:\")\n",
        "    print(model)\n",
        "\n",
        "    # Step 4: Initialize AIMET Quantization Simulation\n",
        "    quant_sim = qsim.QuantizationSimModel(\n",
        "        model,\n",
        "        dummy_input=dummy_input,\n",
        "        rounding_mode='nearest',\n",
        "        default_param_bw=8,\n",
        "        default_output_bw=8\n",
        "    )\n",
        "\n",
        "\n",
        "      # Step 6: Compute Encodings (Calibration Step)\n",
        "    quant_sim.compute_encodings( forward_pass,  dataloader)\n",
        "\n",
        "    print(\"\\nQuantization simulation completed successfully!\")\n",
        "    quant_sim.export(\n",
        "        \"/content\" , \"quantized\" ,  dummy_input = torch.rand(1, 3, 640, 640).to(device)\n",
        "\n",
        "    )\n",
        "    return quant_sim.model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "root_dir = \"/content/datasets/coco/images/val2017\"  # Change this to your COCO images path\n",
        "annotation_file = \"/content/datasets/coco/annotations/instances_val2017.json\"  # Change to your annotation file\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create Dataset\n",
        "coco_dataset = COCODataset(root_dir, annotation_file, transform=transform)\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(coco_dataset, batch_size= 64, shuffle=True, collate_fn= my_collate_fn)\n",
        "\n",
        "# Test DataLoader\n",
        "\n",
        "print(\"Print DataLoader\" + str(len(dataloader)))\n",
        "\n",
        "\n",
        "\n",
        "# Load the model\n",
        "model_path = \"/content/yolov5/yolov5s.pt\"\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, force_reload=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Apply AIMET quantization\n",
        "# quantized_model = apply_aimet_quantization(model,  dataloader)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_14EWAKko04P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44d5c900-90f1-4eee-aaaa-a50f270bc68d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.58s)\n",
            "creating index...\n",
            "index created!\n",
            "Print DataLoader79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 v7.0-398-g5cdad892 Python-3.11.11 torch-2.5.1+cu124 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, targets in dataloader:\n",
        "    print(\"Batch Image Shape:\", images.shape)  # Expect (batch_size, 3, 640, 640)\n",
        "    print(\"Number of Targets in Batch:\", len(targets))  # Should be batch_size\n",
        "    print(\"First Target Tensor Shape:\", targets[0].shape)  # Variable size\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "curvbws3vHLd",
        "outputId": "1ab4101e-0d76-4508-dd30-f78d0905bed6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Image Shape: torch.Size([64, 3, 640, 640])\n",
            "Number of Targets in Batch: 64\n",
            "First Target Tensor Shape: torch.Size([3, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vScmdh-fkhLd",
        "outputId": "1f512ad1-0e74-45de-d86d-556542573e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = apply_aimet_quantization(model,  dataloader , device=device)"
      ],
      "metadata": {
        "id": "c_A7RSjAvZaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9180bc-37e8-402e-ce5c-948a3dcf2519",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model before BatchNorm folding:\n",
            "AutoShape(\n",
            "  (model): DetectMultiBackend(\n",
            "    (model): DetectionModel(\n",
            "      (model): Sequential(\n",
            "        (0): Conv(\n",
            "          (conv): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv(\n",
            "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): Conv(\n",
            "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (4): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "            (1): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (5): Conv(\n",
            "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (6): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "            (1): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "            (2): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (7): Conv(\n",
            "          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (8): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (9): SPPF(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
            "        )\n",
            "        (10): Conv(\n",
            "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (11): Upsample(scale_factor=2.0, mode='nearest')\n",
            "        (12): Concat()\n",
            "        (13): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (14): Conv(\n",
            "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (15): Upsample(scale_factor=2.0, mode='nearest')\n",
            "        (16): Concat()\n",
            "        (17): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (18): Conv(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (19): Concat()\n",
            "        (20): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (21): Conv(\n",
            "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (22): Concat()\n",
            "        (23): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (24): Detect(\n",
            "          (m): ModuleList(\n",
            "            (0): Conv2d(128, 255, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (1): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (2): Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model after BatchNorm folding:\n",
            "AutoShape(\n",
            "  (model): DetectMultiBackend(\n",
            "    (model): DetectionModel(\n",
            "      (model): Sequential(\n",
            "        (0): Conv(\n",
            "          (conv): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2))\n",
            "          (bn): Identity()\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv(\n",
            "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (bn): Identity()\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (2): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): Conv(\n",
            "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (bn): Identity()\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (4): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "            (1): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (5): Conv(\n",
            "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (bn): Identity()\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (6): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "            (1): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "            (2): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (7): Conv(\n",
            "          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (bn): Identity()\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (8): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (9): SPPF(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
            "        )\n",
            "        (10): Conv(\n",
            "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (bn): Identity()\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (11): Upsample(scale_factor=2.0, mode='nearest')\n",
            "        (12): Concat()\n",
            "        (13): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (14): Conv(\n",
            "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (bn): Identity()\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (15): Upsample(scale_factor=2.0, mode='nearest')\n",
            "        (16): Concat()\n",
            "        (17): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (18): Conv(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (bn): Identity()\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (19): Concat()\n",
            "        (20): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (21): Conv(\n",
            "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (bn): Identity()\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (22): Concat()\n",
            "        (23): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (bn): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (bn): Identity()\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (24): Detect(\n",
            "          (m): ModuleList(\n",
            "            (0): Conv2d(128, 255, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (1): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (2): Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-2a0161b5a5f2>:64: DeprecationWarning: \u001b[31;21mThe default value of 'quant_scheme' has changed from 'QuantScheme.post_training_tf_enhanced' to 'QuantScheme.training_range_learning_with_tf_init' since aimet-torch==2.0.0. If you wish to maintain the legacy default behavior, please explicitly pass 'quant_scheme=QuantScheme.post_training_tf_enhanced'\u001b[0m\n",
            "  quant_sim = qsim.QuantizationSimModel(\n",
            "<ipython-input-47-2a0161b5a5f2>:64: DeprecationWarning: \u001b[31;21mPassing rounding_mode='nearest' is no longer needed and will be deprecated soon in the later versions.\u001b[0m\n",
            "  quant_sim = qsim.QuantizationSimModel(\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-09 15:22:43,677 - ConnectedGraph - WARNING - Unable to isolate model outputs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-09 15:22:46,305 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.11/dist-packages/aimet_common/quantsim_config/default_config.json\n",
            "2025-03-09 15:22:46,553 - Quant - INFO - Unsupported op type Squeeze\n",
            "2025-03-09 15:22:46,561 - Quant - INFO - Unsupported op type Mean\n",
            "2025-03-09 15:22:46,602 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Quantization simulation completed successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:688: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-09 15:32:27,265 - Utils - WARNING - end-marker seen without passing start-marker for 'model.model.model.12', continue to use parent context 'model.model'\n",
            "2025-03-09 15:32:27,268 - Utils - WARNING - end-marker seen without passing start-marker for 'model.model.model.16', continue to use parent context 'model.model'\n",
            "2025-03-09 15:32:27,271 - Utils - WARNING - end-marker seen without passing start-marker for 'model.model.model.19', continue to use parent context 'model.model'\n",
            "2025-03-09 15:32:27,274 - Utils - WARNING - end-marker seen without passing start-marker for 'model.model.model.22', continue to use parent context 'model.model'\n",
            "2025-03-09 15:32:27,278 - Utils - WARNING - end-marker seen without passing start-marker for 'model.model.model.24', continue to use parent context '<model>'\n",
            "2025-03-09 15:32:27,298 - Utils - WARNING - end-marker seen without passing start-marker for 'model.model.model.12', continue to use parent context 'model.model'\n",
            "2025-03-09 15:32:27,301 - Utils - WARNING - end-marker seen without passing start-marker for 'model.model.model.16', continue to use parent context 'model.model'\n",
            "2025-03-09 15:32:27,305 - Utils - WARNING - end-marker seen without passing start-marker for 'model.model.model.19', continue to use parent context 'model.model'\n",
            "2025-03-09 15:32:27,307 - Utils - WARNING - end-marker seen without passing start-marker for 'model.model.model.22', continue to use parent context 'model.model'\n",
            "2025-03-09 15:32:27,313 - Utils - WARNING - end-marker seen without passing start-marker for 'model.model.model.24', continue to use parent context '<model>'\n",
            "2025-03-09 15:32:27,358 - Utils - INFO - successfully created onnx model with 185/367 node names updated\n",
            "2025-03-09 15:32:27,708 - Quant - INFO - layer_name: model.model.model.9.m, has multiple output onnx ops: 3,[#0.0.end:1,#0-1.end:1,#0-2.end:1]\n",
            "2025-03-09 15:32:27,711 - Quant - WARNING - number of output quantizers: 1 available for layer: model.model.model.9.m doesn't match with number of output tensors: 3\n",
            "2025-03-09 15:32:27,758 - Quant - WARNING - The following layers were not found in the exported onnx model. Encodings for these layers will not appear in the exported encodings file, however it will continue to exist in torch encoding file:\n",
            "['model.model.model.12', 'model.model.model.16', 'model.model.model.19', 'model.model.model.22']\n",
            "This can be due to several reasons:\n",
            "\t- The layer is set to quantize with float datatype, but was not exercised in compute encodings. Not an issue if the layer is not meant to be run.\n",
            "\t- The layer has valid encodings but was not seen while exporting to onnx using the dummy input provided in sim.export(). Ensure that the dummy input covers all layers.\n",
            "2025-03-09 15:32:27,759 - Quant - INFO - Layers excluded from quantization: []\n",
            "2025-03-09 15:32:27,768 - Quant - WARNING - \u001b[31;21mQuantsim export will stop exporting encodings for saving and loading in a future AIMET release.\n",
            "To export encodings for saving and loading, use QuantizationSimModel's save_encodings_to_json() utility instead.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(quantized_model)"
      ],
      "metadata": {
        "id": "N-Lx7x9JYWuz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4ea6ac-da18-4c6e-8a71-b4cab2389cac",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoShape(\n",
            "  (model): DetectMultiBackend(\n",
            "    (model): DetectionModel(\n",
            "      (model): Sequential(\n",
            "        (0): Conv(\n",
            "          (conv): QuantizedConv2d(\n",
            "            3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2)\n",
            "            (param_quantizers): ModuleDict(\n",
            "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "              (bias): None\n",
            "            )\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "          (act): QuantizedSiLU(\n",
            "            inplace=True\n",
            "            (param_quantizers): ModuleDict()\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): Conv(\n",
            "          (conv): QuantizedConv2d(\n",
            "            32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
            "            (param_quantizers): ModuleDict(\n",
            "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "              (bias): None\n",
            "            )\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "          (act): QuantizedSiLU(\n",
            "            inplace=True\n",
            "            (param_quantizers): ModuleDict()\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              64, 32, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              64, 32, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              64, 64, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  32, 32, kernel_size=(1, 1), stride=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): Conv(\n",
            "          (conv): QuantizedConv2d(\n",
            "            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
            "            (param_quantizers): ModuleDict(\n",
            "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "              (bias): None\n",
            "            )\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "          (act): QuantizedSiLU(\n",
            "            inplace=True\n",
            "            (param_quantizers): ModuleDict()\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (4): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              128, 64, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              128, 64, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              128, 128, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  64, 64, kernel_size=(1, 1), stride=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (1): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  64, 64, kernel_size=(1, 1), stride=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (5): Conv(\n",
            "          (conv): QuantizedConv2d(\n",
            "            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
            "            (param_quantizers): ModuleDict(\n",
            "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "              (bias): None\n",
            "            )\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "          (act): QuantizedSiLU(\n",
            "            inplace=True\n",
            "            (param_quantizers): ModuleDict()\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (6): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              256, 128, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              256, 128, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  128, 128, kernel_size=(1, 1), stride=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (1): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  128, 128, kernel_size=(1, 1), stride=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (2): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  128, 128, kernel_size=(1, 1), stride=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (7): Conv(\n",
            "          (conv): QuantizedConv2d(\n",
            "            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
            "            (param_quantizers): ModuleDict(\n",
            "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "              (bias): None\n",
            "            )\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "          (act): QuantizedSiLU(\n",
            "            inplace=True\n",
            "            (param_quantizers): ModuleDict()\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (8): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              512, 256, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              512, 256, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              512, 512, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (9): SPPF(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              512, 256, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              1024, 512, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (m): QuantizedMaxPool2d(\n",
            "            kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False\n",
            "            (param_quantizers): ModuleDict()\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (10): Conv(\n",
            "          (conv): QuantizedConv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (param_quantizers): ModuleDict(\n",
            "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "              (bias): None\n",
            "            )\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "          (act): QuantizedSiLU(\n",
            "            inplace=True\n",
            "            (param_quantizers): ModuleDict()\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (11): QuantizedUpsample(\n",
            "          scale_factor=2.0, mode='nearest'\n",
            "          (param_quantizers): ModuleDict()\n",
            "          (input_quantizers): ModuleList(\n",
            "            (0): None\n",
            "          )\n",
            "          (output_quantizers): ModuleList(\n",
            "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "          )\n",
            "        )\n",
            "        (12): QuantizedConcat(\n",
            "          (param_quantizers): ModuleDict()\n",
            "          (input_quantizers): ModuleList(\n",
            "            (0): None\n",
            "          )\n",
            "          (output_quantizers): ModuleList(\n",
            "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "          )\n",
            "        )\n",
            "        (13): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              512, 128, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              512, 128, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  128, 128, kernel_size=(1, 1), stride=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (14): Conv(\n",
            "          (conv): QuantizedConv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (param_quantizers): ModuleDict(\n",
            "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "              (bias): None\n",
            "            )\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "          (act): QuantizedSiLU(\n",
            "            inplace=True\n",
            "            (param_quantizers): ModuleDict()\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (15): QuantizedUpsample(\n",
            "          scale_factor=2.0, mode='nearest'\n",
            "          (param_quantizers): ModuleDict()\n",
            "          (input_quantizers): ModuleList(\n",
            "            (0): None\n",
            "          )\n",
            "          (output_quantizers): ModuleList(\n",
            "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "          )\n",
            "        )\n",
            "        (16): QuantizedConcat(\n",
            "          (param_quantizers): ModuleDict()\n",
            "          (input_quantizers): ModuleList(\n",
            "            (0): None\n",
            "          )\n",
            "          (output_quantizers): ModuleList(\n",
            "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "          )\n",
            "        )\n",
            "        (17): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              256, 64, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              256, 64, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              128, 128, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  64, 64, kernel_size=(1, 1), stride=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (18): Conv(\n",
            "          (conv): QuantizedConv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
            "            (param_quantizers): ModuleDict(\n",
            "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "              (bias): None\n",
            "            )\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "          (act): QuantizedSiLU(\n",
            "            inplace=True\n",
            "            (param_quantizers): ModuleDict()\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (19): QuantizedConcat(\n",
            "          (param_quantizers): ModuleDict()\n",
            "          (input_quantizers): ModuleList(\n",
            "            (0): None\n",
            "          )\n",
            "          (output_quantizers): ModuleList(\n",
            "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "          )\n",
            "        )\n",
            "        (20): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              256, 128, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              256, 128, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  128, 128, kernel_size=(1, 1), stride=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (21): Conv(\n",
            "          (conv): QuantizedConv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
            "            (param_quantizers): ModuleDict(\n",
            "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "              (bias): None\n",
            "            )\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "          (act): QuantizedSiLU(\n",
            "            inplace=True\n",
            "            (param_quantizers): ModuleDict()\n",
            "            (input_quantizers): ModuleList(\n",
            "              (0): None\n",
            "            )\n",
            "            (output_quantizers): ModuleList(\n",
            "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (22): QuantizedConcat(\n",
            "          (param_quantizers): ModuleDict()\n",
            "          (input_quantizers): ModuleList(\n",
            "            (0): None\n",
            "          )\n",
            "          (output_quantizers): ModuleList(\n",
            "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "          )\n",
            "        )\n",
            "        (23): C3(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              512, 256, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              512, 256, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (cv3): Conv(\n",
            "            (conv): QuantizedConv2d(\n",
            "              512, 512, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (act): QuantizedSiLU(\n",
            "              inplace=True\n",
            "              (param_quantizers): ModuleDict()\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (m): Sequential(\n",
            "            (0): Bottleneck(\n",
            "              (cv1): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (cv2): Conv(\n",
            "                (conv): QuantizedConv2d(\n",
            "                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "                  (param_quantizers): ModuleDict(\n",
            "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                    (bias): None\n",
            "                  )\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "                (act): QuantizedSiLU(\n",
            "                  inplace=True\n",
            "                  (param_quantizers): ModuleDict()\n",
            "                  (input_quantizers): ModuleList(\n",
            "                    (0): None\n",
            "                  )\n",
            "                  (output_quantizers): ModuleList(\n",
            "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (24): Detect(\n",
            "          (m): ModuleList(\n",
            "            (0): QuantizedConv2d(\n",
            "              128, 255, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (1): QuantizedConv2d(\n",
            "              256, 255, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "            (2): QuantizedConv2d(\n",
            "              512, 255, kernel_size=(1, 1), stride=(1, 1)\n",
            "              (param_quantizers): ModuleDict(\n",
            "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
            "                (bias): None\n",
            "              )\n",
            "              (input_quantizers): ModuleList(\n",
            "                (0): None\n",
            "              )\n",
            "              (output_quantizers): ModuleList(\n",
            "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxconverter_common"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "jQtsqBYrrUTh",
        "outputId": "7ed3ae9e-152b-4bb1-88d3-13e43a36fcc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxconverter_common\n",
            "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from onnxconverter_common) (1.26.4)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from onnxconverter_common) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxconverter_common) (24.2)\n",
            "Collecting protobuf==3.20.2 (from onnxconverter_common)\n",
            "  Downloading protobuf-3.20.2-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.2-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, onnxconverter_common\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnxconverter_common-1.14.0 protobuf-3.20.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "19490146f8604588a63505508af2bfe7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxconverter_common import float16\n",
        "import onnx\n",
        "\n",
        "# Load ONNX model\n",
        "model = onnx.load(\"/content/quantized.onnx\")\n",
        "\n",
        "# Convert model to float16\n",
        "model_fp16 = float16.convert_float_to_float16(model)\n",
        "\n",
        "# Save the new model\n",
        "onnx.save(model_fp16, \"/content/quantized_model_fp16.onnx\")\n"
      ],
      "metadata": {
        "id": "JNQWUeoZrRxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b932b116-a1d5-41c6-9136-7f9a561856ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[autoreload of google.protobuf.descriptor failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
            "    update_generic(old_obj, new_obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
            "    update(a, b)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 317, in update_class\n",
            "    update_instances(old, new)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 280, in update_instances\n",
            "    ref.__class__ = new\n",
            "    ^^^^^^^^^^^^^\n",
            "TypeError: __class__ assignment only supported for mutable types or ModuleType subclasses\n",
            "]\n",
            "[autoreload of google.protobuf.pyext.cpp_message failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
            "    module = reload(module)\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/imp.py\", line 315, in reload\n",
            "    return importlib.reload(module)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 169, in reload\n",
            "    _bootstrap._exec(spec, module)\n",
            "  File \"<frozen importlib._bootstrap>\", line 621, in _exec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/protobuf/pyext/cpp_message.py\", line 39, in <module>\n",
            "    from google.protobuf.pyext import _message\n",
            "ImportError: cannot import name '_message' from 'google.protobuf.pyext' (/usr/local/lib/python3.11/dist-packages/google/protobuf/pyext/__init__.py)\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate YOLOv5s on COCO val\n",
        "!python val.py --weights /content/quantized_model_fp16.onnx --data coco.yaml --img 640 --half"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnLCOel-iiHN",
        "outputId": "05a634e0-4e87-47eb-857d-377cc838d76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/coco.yaml, weights=['/content/quantized_model_fp16.onnx'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 🚀 v7.0-398-g5cdad892 Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Loading /content/quantized_model_fp16.onnx for ONNX Runtime inference...\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnxruntime-gpu'] not found, attempting AutoUpdate...\n",
            "Collecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting coloredlogs (from onnxruntime-gpu)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (3.20.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Downloading onnxruntime_gpu-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (280.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.8/280.8 MB 163.1 MB/s eta 0:00:00\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 184.0 MB/s eta 0:00:00\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 199.5 MB/s eta 0:00:00\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-gpu-1.21.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 14.1s, installed 1 package: ['onnxruntime-gpu']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\u001b[0;93m2025-03-08 07:06:46.626605641 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Floor node '/model/marked_module/model/marked_module/marked_module.11/marked_module/Floor_1'\u001b[m\n",
            "\u001b[0;93m2025-03-08 07:06:46.626853462 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Floor node '/model/marked_module/model/marked_module/marked_module.15/marked_module/Floor_1'\u001b[m\n",
            "\u001b[0;93m2025-03-08 07:06:46.630043665 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Floor node '/model/marked_module/model/marked_module/marked_module.11/marked_module/Floor_1'\u001b[m\n",
            "\u001b[0;93m2025-03-08 07:06:46.630083988 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Floor node '/model/marked_module/model/marked_module/marked_module.15/marked_module/Floor_1'\u001b[m\n",
            "\u001b[0;93m2025-03-08 07:06:46.642303352 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 2 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
            "Forcing --batch-size 1 square inference (1,3,640,640) for non-PyTorch models\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco/val2017.cache... 4952 images, 48 backgrounds, 0 corrupt: 100% 5000/5000 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 5000/5000 [02:22<00:00, 35.14it/s]\n",
            "                   all       5000      36335      0.671      0.514       0.56      0.362\n",
            "Speed: 0.3ms pre-process, 10.7ms inference, 2.0ms NMS per image at shape (1, 3, 640, 640)\n",
            "\n",
            "Evaluating pycocotools mAP... saving runs/val/exp2/quantized_model_fp16_predictions.json...\n",
            "loading annotations into memory...\n",
            "Done (t=0.90s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=6.56s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=67.61s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=13.42s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.365\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.566\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.392\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.414\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.305\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.508\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.559\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.371\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.621\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.712\n",
            "Results saved to \u001b[1mruns/val/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoQuant"
      ],
      "metadata": {
        "id": "yBk9THa1mXOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from aimet_torch.auto_quant import AutoQuant\n",
        "from yolov5.val import run as yolo_val  # Import YOLOv5 validation\n",
        "from yolov5.models.common import DetectMultiBackend\n",
        "\n",
        "# Define the evaluation callback function\n",
        "def eval_callback(model: torch.nn.Module, dummy_input: torch.Tensor = None) -> float:\n",
        "    \"\"\"\n",
        "    Evaluation function for AutoQuant.\n",
        "    Runs YOLOv5 validation and returns mAP score.\n",
        "    \"\"\"\n",
        "    print(\"Running YOLOv5 evaluation...\")\n",
        "\n",
        "    # Set device\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Run YOLOv5 validation with the given model\n",
        "    results = yolo_val(model=model, data='data/coco.yaml', imgsz=640, batch_size=16, device=device)\n",
        "\n",
        "    # Extract mAP (mean Average Precision) score\n",
        "    mAP50_95 = results[2]  # mAP@[.5:.95]\n",
        "\n",
        "    print(f\"Validation Completed. mAP@[.5:.95]: {mAP50_95:.4f}\")\n",
        "\n",
        "    return mAP50_95  # Return the evaluation metric for AutoQuant\n"
      ],
      "metadata": {
        "id": "lhwy23BhnHxL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from aimet_torch.auto_quant import AutoQuant\n",
        "auto_quant = AutoQuant(model, dummy_input=dummy_input, data_loader=dataloader,\n",
        "                           eval_callback=eval_callback, results_dir=f\"/content/auto_quant_results\", model_prepare_required=False)\n",
        "\n",
        "    # Run inference before optimization\n",
        "sim, initial_accuracy = auto_quant.run_inference()\n"
      ],
      "metadata": {
        "id": "ESzo2CY80j5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afdac74-409c-489c-a41d-85da0576c3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "- Batchnorm Folding/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "\\ Batchnorm Folding/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/ Batchnorm Folding/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "- Batchnorm Folding/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/ Batchnorm Folding/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "\\ Batchnorm Folding/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "| Batchnorm Folding/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-09 16:14:33,728 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.11/dist-packages/aimet_common/quantsim_config/default_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-09 16:14:33,765 - Quant - INFO - Unsupported op type Squeeze\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-09 16:14:33,771 - Quant - INFO - Unsupported op type Mean\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-09 16:14:33,799 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\\ Batchnorm Folding\n",
            "- Batchnorm Folding/content/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "- Batchnorm Folding"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from aimet_torch.auto_quant import AutoQuant\n",
        "from yolov5.models.yolo import Model  # Load YOLOv5 model directly\n",
        "from yolov5.val import run as yolo_val  # Import YOLOv5 validation\n",
        "\n",
        "# 1️⃣ Load YOLOv5 model manually (without DetectMultiBackend)\n",
        "weights = '/content/yolov5/yolov5s.pt'  # Change to your model's path\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the model using YOLOv5's Model class\n",
        "model = Model(cfg='/content/yolov5/models/yolov5s.yaml')  # Load model from YOLOv5 config\n",
        "ckpt = torch.load(weights, map_location=device )  # Load weights\n",
        "model.load_state_dict(ckpt['model'].state_dict() , strict=False)  # Apply weights\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 2️⃣ Define the evaluation callback function\n",
        "def eval_callback(model: torch.nn.Module, dummy_input: torch.Tensor = None) -> float:\n",
        "    \"\"\"\n",
        "    Evaluation function for AutoQuant.\n",
        "    Runs YOLOv5 validation and returns mAP score.\n",
        "    \"\"\"\n",
        "    print(\"Running YOLOv5 evaluation...\")\n",
        "\n",
        "    # Run YOLOv5 validation with the given model\n",
        "    results = yolo_val(model=model, data='/content/yolov5/data/coco.yaml', imgsz=640, batch_size=16, device=device)\n",
        "\n",
        "    # Extract mAP (mean Average Precision) score\n",
        "    mAP50_95 = results[2]  # mAP@[.5:.95]\n",
        "\n",
        "    print(f\"Validation Completed. mAP@[.5:.95]: {mAP50_95:.4f}\")\n",
        "\n",
        "    return mAP50_95  # Return the evaluation metric for AutoQuant\n",
        "\n",
        "# 3️⃣ Set up AutoQuant\n",
        "dummy_input = torch.randn(1, 3, 640, 640).to(device)  # Define a sample input\n"
      ],
      "metadata": {
        "id": "_SsgDSXF0j15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d51d5e-be5c-4212-de2c-a482f1b037bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    229245  yolov5.models.yolo.Detect               [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "YOLOv5s summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_callback(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Gn28aCpl7cQd",
        "outputId": "f914a183-6b75-49ba-fd69-253a810bee92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running YOLOv5 evaluation...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'get'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-917ec1d51e65>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-5598854b178f>\u001b[0m in \u001b[0;36meval_callback\u001b[0;34m(model, dummy_input)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Run YOLOv5 validation with the given model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/coco.yaml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Extract mAP (mean Average Precision) score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/yolov5/val.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(data, weights, batch_size, imgsz, conf_thres, iou_thres, max_det, task, device, workers, single_cls, augment, verbose, save_txt, save_hybrid, save_conf, save_json, project, name, exist_ok, half, dnn, model, dataloader, save_dir, plots, callbacks, compute_loss)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0mcuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0mis_coco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"coco{os.sep}val2017.txt\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# COCO dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m     \u001b[0mnc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msingle_cls\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0miouv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# iou vector for mAP@0.5:0.95\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aimet_torch.batch_norm_fold import fold_all_batch_norms\n",
        "fold_all_batch_norms(model , input_shapes=(1,3,640, 640))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L4l-KYFq4KOr",
        "outputId": "efd74144-fafe-41ca-80fb-c51f04408247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/yolov5/models/yolo.py:102: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if self.grid[i].shape[2:4] != x[i].shape[2:4]:  # Remove 'self.dynamic or'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py:1278: TracerWarning: Encountering a list at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.\n",
            "  module._c._create_method_from_trace(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'1'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-a30782e647c9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maimet_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm_fold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfold_all_batch_norms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfold_all_batch_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/_base/batch_norm_fold.py\u001b[0m in \u001b[0;36mfold_all_batch_norms_to_weight\u001b[0;34m(cls, model, input_shapes, dummy_input)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0minp_tensor_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummy_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mconnected_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectedGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_tensor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mconv_bn_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_conv_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_to_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_all_batch_norms_to_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnected_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, model_input)\u001b[0m\n\u001b[1;32m    168\u001b[0m                                                                        lambda m: inspect.isclass(m) and issubclass(m,\n\u001b[1;32m    169\u001b[0m                                                                                                                    torch.nn.Module)))\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aimet_defined_modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\u001b[0m in \u001b[0;36m_construct_graph\u001b[0;34m(self, model, model_input)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mmodule_tensor_shapes_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectedGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_module_tensor_shapes_lookup_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mjit_trace_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_top_level_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recover_input_output_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_connected_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\u001b[0m in \u001b[0;36m_parse_top_level_trace\u001b[0;34m(self, trace, model)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0moutput_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_level_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_to_jit_trace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_to_jit_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;31m# pylint: disable=too-many-branches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\u001b[0m in \u001b[0;36m_parse_trace_graph\u001b[0;34m(self, trace, model, output_map, higher_level_inputs, module_to_jit_trace, elementwise_info)\u001b[0m\n\u001b[1;32m    395\u001b[0m                 \u001b[0;31m# In the case of parsing the interior of an elementwise op, we never expect to see any CallMethod nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0melementwise_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                 submodule_outputs = self._parse_callmethod_node(node, trace, node_name_to_module,\n\u001b[0m\u001b[1;32m    398\u001b[0m                                                                 \u001b[0mnode_name_to_subgraph_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                                                                 module_to_jit_trace)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\u001b[0m in \u001b[0;36m_parse_callmethod_node\u001b[0;34m(self, node, trace, node_name_to_module, node_name_to_subgraph_model, output_map, residing_module, module_to_jit_trace)\u001b[0m\n\u001b[1;32m    583\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             submodule_outputs = self._parse_trace_graph(subgraph_trace, subgraph_model, output_map, inputs[1:],\n\u001b[0m\u001b[1;32m    586\u001b[0m                                                         \u001b[0mmodule_to_jit_trace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_to_jit_trace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                                                         elementwise_info=elementwise_info)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\u001b[0m in \u001b[0;36m_parse_trace_graph\u001b[0;34m(self, trace, model, output_map, higher_level_inputs, module_to_jit_trace, elementwise_info)\u001b[0m\n\u001b[1;32m    395\u001b[0m                 \u001b[0;31m# In the case of parsing the interior of an elementwise op, we never expect to see any CallMethod nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0melementwise_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                 submodule_outputs = self._parse_callmethod_node(node, trace, node_name_to_module,\n\u001b[0m\u001b[1;32m    398\u001b[0m                                                                 \u001b[0mnode_name_to_subgraph_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                                                                 module_to_jit_trace)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\u001b[0m in \u001b[0;36m_parse_callmethod_node\u001b[0;34m(self, node, trace, node_name_to_module, node_name_to_subgraph_model, output_map, residing_module, module_to_jit_trace)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;31m# Op is a leaf level module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mop_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_op_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_name_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_multi_output_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresiding_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_name_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_products_for_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '1'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traced_model = torch.jit.trace(model, torch.randn(1, 3, 640, 640))\n",
        "print(traced_model.graph)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjsQGjJ-4oYD",
        "outputId": "e6049426-a1ba-41b8-e780-2908853bab0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/yolov5/models/yolo.py:102: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if self.grid[i].shape[2:4] != x[i].shape[2:4]:  # Remove 'self.dynamic or'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py:1278: TracerWarning: Encountering a list at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.\n",
            "  module._c._create_method_from_trace(\n",
            "/content/yolov5/models/yolo.py:102: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if self.grid[i].shape[2:4] != x[i].shape[2:4]:  # Remove 'self.dynamic or'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py:1278: TracerWarning: Encountering a list at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.\n",
            "  module._c._create_method_from_trace(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "graph(%self.1 : __torch__.yolov5.models.yolo.___torch_mangle_3326.DetectionModel,\n",
            "      %x : Float(1, 3, 640, 640, strides=[1228800, 409600, 640, 1], requires_grad=0, device=cpu)):\n",
            "  %model : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_24 : __torch__.yolov5.models.yolo.___torch_mangle_3324.Detect = prim::GetAttr[name=\"24\"](%model)\n",
            "  %model.49 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_23 : __torch__.models.common.___torch_mangle_3319.C3 = prim::GetAttr[name=\"23\"](%model.49)\n",
            "  %model.47 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_22 : __torch__.models.common.___torch_mangle_3296.Concat = prim::GetAttr[name=\"22\"](%model.47)\n",
            "  %model.45 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_21 : __torch__.models.common.___torch_mangle_3295.Conv = prim::GetAttr[name=\"21\"](%model.45)\n",
            "  %model.43 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_20 : __torch__.models.common.___torch_mangle_3291.C3 = prim::GetAttr[name=\"20\"](%model.43)\n",
            "  %model.41 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_19 : __torch__.models.common.___torch_mangle_3268.Concat = prim::GetAttr[name=\"19\"](%model.41)\n",
            "  %model.39 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_18 : __torch__.models.common.___torch_mangle_3267.Conv = prim::GetAttr[name=\"18\"](%model.39)\n",
            "  %model.37 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_17 : __torch__.models.common.___torch_mangle_3263.C3 = prim::GetAttr[name=\"17\"](%model.37)\n",
            "  %model.35 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_16 : __torch__.models.common.___torch_mangle_3240.Concat = prim::GetAttr[name=\"16\"](%model.35)\n",
            "  %model.33 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_15 : __torch__.torch.nn.modules.upsampling.___torch_mangle_3239.Upsample = prim::GetAttr[name=\"15\"](%model.33)\n",
            "  %model.31 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_14 : __torch__.models.common.___torch_mangle_3238.Conv = prim::GetAttr[name=\"14\"](%model.31)\n",
            "  %model.29 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_13 : __torch__.models.common.___torch_mangle_3234.C3 = prim::GetAttr[name=\"13\"](%model.29)\n",
            "  %model.27 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_12 : __torch__.models.common.___torch_mangle_3211.Concat = prim::GetAttr[name=\"12\"](%model.27)\n",
            "  %model.25 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_11 : __torch__.torch.nn.modules.upsampling.___torch_mangle_3210.Upsample = prim::GetAttr[name=\"11\"](%model.25)\n",
            "  %model.23 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_10 : __torch__.models.common.___torch_mangle_3209.Conv = prim::GetAttr[name=\"10\"](%model.23)\n",
            "  %model.21 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_9 : __torch__.models.common.___torch_mangle_3205.SPPF = prim::GetAttr[name=\"9\"](%model.21)\n",
            "  %model.19 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_8 : __torch__.models.common.___torch_mangle_3195.C3 = prim::GetAttr[name=\"8\"](%model.19)\n",
            "  %model.17 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_7 : __torch__.models.common.___torch_mangle_3172.Conv = prim::GetAttr[name=\"7\"](%model.17)\n",
            "  %model.15 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_6 : __torch__.models.common.___torch_mangle_3168.C3 = prim::GetAttr[name=\"6\"](%model.15)\n",
            "  %model.13 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_5 : __torch__.models.common.___torch_mangle_3127.Conv = prim::GetAttr[name=\"5\"](%model.13)\n",
            "  %model.11 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_4 : __torch__.models.common.___torch_mangle_3123.C3 = prim::GetAttr[name=\"4\"](%model.11)\n",
            "  %model.9 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_3 : __torch__.models.common.___torch_mangle_3091.Conv = prim::GetAttr[name=\"3\"](%model.9)\n",
            "  %model.7 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_2.1 : __torch__.models.common.___torch_mangle_3087.C3 = prim::GetAttr[name=\"2\"](%model.7)\n",
            "  %model.5 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_1.1 : __torch__.models.common.___torch_mangle_3064.Conv = prim::GetAttr[name=\"1\"](%model.5)\n",
            "  %model.3 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_23.1 : __torch__.models.common.___torch_mangle_3319.C3 = prim::GetAttr[name=\"23\"](%model.3)\n",
            "  %m.1 : __torch__.torch.nn.modules.container.___torch_mangle_3318.Sequential = prim::GetAttr[name=\"m\"](%_23.1)\n",
            "  %_0.3 : __torch__.models.common.___torch_mangle_3317.Bottleneck = prim::GetAttr[name=\"0\"](%m.1)\n",
            "  %cv2.1 : __torch__.models.common.___torch_mangle_3316.Conv = prim::GetAttr[name=\"cv2\"](%_0.3)\n",
            "  %act.1 : __torch__.torch.nn.modules.activation.___torch_mangle_3315.SiLU = prim::GetAttr[name=\"act\"](%cv2.1)\n",
            "  %model.1 : __torch__.torch.nn.modules.container.___torch_mangle_3325.Sequential = prim::GetAttr[name=\"model\"](%self.1)\n",
            "  %_0.1 : __torch__.models.common.___torch_mangle_3060.Conv = prim::GetAttr[name=\"0\"](%model.1)\n",
            "  %4268 : Tensor = prim::CallMethod[name=\"forward\"](%_0.1, %act.1, %x)\n",
            "  %4269 : Tensor = prim::CallMethod[name=\"forward\"](%_1.1, %act.1, %4268)\n",
            "  %4270 : Tensor = prim::CallMethod[name=\"forward\"](%_2.1, %act.1, %4269)\n",
            "  %4271 : Tensor = prim::CallMethod[name=\"forward\"](%_3, %act.1, %4270)\n",
            "  %4272 : Tensor = prim::CallMethod[name=\"forward\"](%_4, %act.1, %4271)\n",
            "  %4273 : Tensor = prim::CallMethod[name=\"forward\"](%_5, %act.1, %4272)\n",
            "  %4274 : Tensor = prim::CallMethod[name=\"forward\"](%_6, %act.1, %4273)\n",
            "  %4275 : Tensor = prim::CallMethod[name=\"forward\"](%_7, %act.1, %4274)\n",
            "  %4276 : Tensor = prim::CallMethod[name=\"forward\"](%_8, %act.1, %4275)\n",
            "  %4277 : Tensor = prim::CallMethod[name=\"forward\"](%_9, %act.1, %4276)\n",
            "  %4278 : Tensor = prim::CallMethod[name=\"forward\"](%_10, %act.1, %4277)\n",
            "  %4279 : Tensor = prim::CallMethod[name=\"forward\"](%_11, %4278)\n",
            "  %4280 : Tensor = prim::CallMethod[name=\"forward\"](%_12, %4279, %4274)\n",
            "  %4281 : Tensor = prim::CallMethod[name=\"forward\"](%_13, %act.1, %4280)\n",
            "  %4282 : Tensor = prim::CallMethod[name=\"forward\"](%_14, %act.1, %4281)\n",
            "  %4283 : Tensor = prim::CallMethod[name=\"forward\"](%_15, %4282)\n",
            "  %4284 : Tensor = prim::CallMethod[name=\"forward\"](%_16, %4283, %4272)\n",
            "  %4285 : Tensor = prim::CallMethod[name=\"forward\"](%_17, %act.1, %4284)\n",
            "  %4286 : Tensor = prim::CallMethod[name=\"forward\"](%_18, %act.1, %4285)\n",
            "  %4287 : Tensor = prim::CallMethod[name=\"forward\"](%_19, %4286, %4282)\n",
            "  %4288 : Tensor = prim::CallMethod[name=\"forward\"](%_20, %act.1, %4287)\n",
            "  %4289 : Tensor = prim::CallMethod[name=\"forward\"](%_21, %act.1, %4288)\n",
            "  %4290 : Tensor = prim::CallMethod[name=\"forward\"](%_22, %4289, %4278)\n",
            "  %4291 : Tensor = prim::CallMethod[name=\"forward\"](%_23, %4290)\n",
            "  %4292 : (Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name=\"forward\"](%_24, %4285, %4288, %4291)\n",
            "  %4264 : Float(1, 3, 80, 80, 85, strides=[1632000, 544000, 6800, 85, 1], requires_grad=1, device=cpu), %4265 : Float(1, 3, 40, 40, 85, strides=[408000, 136000, 3400, 85, 1], requires_grad=1, device=cpu), %4266 : Float(1, 3, 20, 20, 85, strides=[102000, 34000, 1700, 85, 1], requires_grad=1, device=cpu), %4267 : Float(1, 25200, 85, strides=[2142000, 85, 1], requires_grad=1, device=cpu) = prim::TupleUnpack(%4292)\n",
            "  %3096 : Tensor[] = prim::ListConstruct(%4264, %4265, %4266)\n",
            "  %3097 : (Float(1, 25200, 85, strides=[2142000, 85, 1], requires_grad=1, device=cpu), Tensor[]) = prim::TupleConstruct(%4267, %3096)\n",
            "  return (%3097)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "auto_quant = AutoQuant(\n",
        "    model=model,  # Make sure this is a standard `torch.nn.Module`\n",
        "    dummy_input=dummy_input,\n",
        "    data_loader= dataloader,  # Optional, if not needed\n",
        "    eval_callback=eval_callback,\n",
        "    results_dir=\"/content/auto_quant_results\",\n",
        "    model_prepare_required=False\n",
        ")\n",
        "\n",
        "# 4️⃣ Run AutoQuant\n",
        "print(\"Running AutoQuant optimization...\")\n",
        "sim, initial_accuracy ,  = auto_quant.run_inference()\n",
        "print(f\"Initial mAP@[.5:.95]: {initial_accuracy:.4f}\")\n",
        "\n",
        "# Save the optimized model\n",
        "torch.save(sim.state_dict(), \"yolov5_autoquant.pth\")\n",
        "print(\"Quantized model saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "70cw0fAY4GgF",
        "outputId": "80c1fc9c-4a6a-46f4-8a87-3e01b128630f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running AutoQuant optimization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\\ Batchnorm Folding/content/yolov5/models/yolo.py:102: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if self.grid[i].shape[2:4] != x[i].shape[2:4]:  # Remove 'self.dynamic or'\n",
            "| Batchnorm Folding/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py:1278: TracerWarning: Encountering a list at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.\n",
            "  module._c._create_method_from_trace(\n",
            "| Batchnorm Folding"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aimet_torch/_base/auto_quant.py\", line 883, in run_inference\n",
            "    model, _ = sess.wrap(self._apply_batchnorm_folding)(model)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aimet_torch/_base/auto_quant.py\", line 475, in wrapper\n",
            "    ret = fn(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aimet_common/cache.py\", line 186, in caching_helper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aimet_torch/_base/auto_quant.py\", line 1088, in _apply_batchnorm_folding\n",
            "    folded_pairs = fold_all_batch_norms(model, None, self.dummy_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aimet_torch/_base/batch_norm_fold.py\", line 190, in fold_all_batch_norms_to_weight\n",
            "    connected_graph = ConnectedGraph(model, inp_tensor_list)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\", line 170, in __init__\n",
            "    self._construct_graph(model, model_input)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\", line 297, in _construct_graph\n",
            "    self._parse_top_level_trace(trace, model)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\", line 339, in _parse_top_level_trace\n",
            "    _ = self._parse_trace_graph(trace, model, output_map, top_level_inputs, module_to_jit_trace=module_to_jit_trace)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\", line 397, in _parse_trace_graph\n",
            "    submodule_outputs = self._parse_callmethod_node(node, trace, node_name_to_module,\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\", line 585, in _parse_callmethod_node\n",
            "    submodule_outputs = self._parse_trace_graph(subgraph_trace, subgraph_model, output_map, inputs[1:],\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\", line 397, in _parse_trace_graph\n",
            "    submodule_outputs = self._parse_callmethod_node(node, trace, node_name_to_module,\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\", line 591, in _parse_callmethod_node\n",
            "    op_type = self.get_op_type(type(node_name_to_module[input_name]))\n",
            "                                    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: '1'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'1'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-ec78787c8852>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 4️⃣ Run AutoQuant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running AutoQuant optimization...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_accuracy\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mauto_quant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Initial mAP@[.5:.95]: {initial_accuracy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/_base/auto_quant.py\u001b[0m in \u001b[0;36mrun_inference\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;31m# Batchnorm Folding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batchnorm Folding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_batchnorm_folding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptq_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 sess.set_ptq_result(model=model,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/_base/auto_quant.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCachedResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_common/cache.py\u001b[0m in \u001b[0;36mcaching_helper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# If caching is disabled, evalaute the result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mworking_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/_base/auto_quant.py\u001b[0m in \u001b[0;36m_apply_batchnorm_folding\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \"\"\"\n\u001b[1;32m   1087\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0mfolded_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfold_all_batch_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolded_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/_base/batch_norm_fold.py\u001b[0m in \u001b[0;36mfold_all_batch_norms_to_weight\u001b[0;34m(cls, model, input_shapes, dummy_input)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0minp_tensor_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummy_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mconnected_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectedGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_tensor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mconv_bn_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_conv_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_to_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_all_batch_norms_to_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnected_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, model_input)\u001b[0m\n\u001b[1;32m    168\u001b[0m                                                                        lambda m: inspect.isclass(m) and issubclass(m,\n\u001b[1;32m    169\u001b[0m                                                                                                                    torch.nn.Module)))\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aimet_defined_modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\u001b[0m in \u001b[0;36m_construct_graph\u001b[0;34m(self, model, model_input)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mmodule_tensor_shapes_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectedGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_module_tensor_shapes_lookup_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mjit_trace_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_top_level_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recover_input_output_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_connected_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\u001b[0m in \u001b[0;36m_parse_top_level_trace\u001b[0;34m(self, trace, model)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0moutput_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_level_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_to_jit_trace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_to_jit_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;31m# pylint: disable=too-many-branches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\u001b[0m in \u001b[0;36m_parse_trace_graph\u001b[0;34m(self, trace, model, output_map, higher_level_inputs, module_to_jit_trace, elementwise_info)\u001b[0m\n\u001b[1;32m    395\u001b[0m                 \u001b[0;31m# In the case of parsing the interior of an elementwise op, we never expect to see any CallMethod nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0melementwise_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                 submodule_outputs = self._parse_callmethod_node(node, trace, node_name_to_module,\n\u001b[0m\u001b[1;32m    398\u001b[0m                                                                 \u001b[0mnode_name_to_subgraph_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                                                                 module_to_jit_trace)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\u001b[0m in \u001b[0;36m_parse_callmethod_node\u001b[0;34m(self, node, trace, node_name_to_module, node_name_to_subgraph_model, output_map, residing_module, module_to_jit_trace)\u001b[0m\n\u001b[1;32m    583\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             submodule_outputs = self._parse_trace_graph(subgraph_trace, subgraph_model, output_map, inputs[1:],\n\u001b[0m\u001b[1;32m    586\u001b[0m                                                         \u001b[0mmodule_to_jit_trace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_to_jit_trace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                                                         elementwise_info=elementwise_info)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\u001b[0m in \u001b[0;36m_parse_trace_graph\u001b[0;34m(self, trace, model, output_map, higher_level_inputs, module_to_jit_trace, elementwise_info)\u001b[0m\n\u001b[1;32m    395\u001b[0m                 \u001b[0;31m# In the case of parsing the interior of an elementwise op, we never expect to see any CallMethod nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0melementwise_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                 submodule_outputs = self._parse_callmethod_node(node, trace, node_name_to_module,\n\u001b[0m\u001b[1;32m    398\u001b[0m                                                                 \u001b[0mnode_name_to_subgraph_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                                                                 module_to_jit_trace)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aimet_torch/meta/connectedgraph.py\u001b[0m in \u001b[0;36m_parse_callmethod_node\u001b[0;34m(self, node, trace, node_name_to_module, node_name_to_subgraph_model, output_map, residing_module, module_to_jit_trace)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;31m# Op is a leaf level module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mop_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_op_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_name_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_multi_output_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresiding_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_name_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_products_for_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '1'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference on Pytorch **"
      ],
      "metadata": {
        "id": "1ShztVeu0YG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from yolov5.models.experimental import attempt_load\n",
        "from yolov5.utils.general import non_max_suppression, coco80_to_coco91_class\n",
        "from yolov5.utils.metrics import ap_per_class\n",
        "from yolov5.utils.torch_utils import select_device\n",
        "from yolov5.utils.general import scale_boxes\n",
        "\n",
        "def validate_yolo(model_path, dataloader, device=\"cuda\", img_size=640, conf_thres=0.001, iou_thres=0.6):\n",
        "    \"\"\"\n",
        "    Validates a YOLOv5 model on a COCO-style dataset and returns performance metrics.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the trained YOLOv5 .pt file.\n",
        "        dataloader (DataLoader): COCO validation DataLoader.\n",
        "        device (str): \"cuda\" or \"cpu\".\n",
        "        img_size (int): Input image size.\n",
        "        conf_thres (float): Confidence threshold for predictions.\n",
        "        iou_thres (float): IoU threshold for non-max suppression.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing mAP scores and other performance metrics.\n",
        "    \"\"\"\n",
        "    # Select device\n",
        "    device = select_device(device)\n",
        "\n",
        "    # Load YOLOv5 model\n",
        "    model = attempt_load(model_path, device=device)\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize lists for storing targets & predictions\n",
        "    seen = 0\n",
        "    targets_all, preds_all = [], []\n",
        "\n",
        "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
        "        images = images.to(device)\n",
        "        targets = [t.to(device) for t in targets]\n",
        "\n",
        "        # Run inference\n",
        "        with torch.no_grad():\n",
        "            preds = model(images)\n",
        "\n",
        "        # Apply Non-Maximum Suppression (NMS)\n",
        "        preds = non_max_suppression(preds, conf_thres, iou_thres)\n",
        "\n",
        "        # Collect targets and predictions\n",
        "        for si, pred in enumerate(preds):\n",
        "            if pred is not None and len(pred):\n",
        "                # Scale boxes to original image size\n",
        "                pred[:, :4] = scale_boxes(images[si].shape[1:], pred[:, :4], (img_size, img_size)).round()\n",
        "                preds_all.append(pred.cpu().numpy())\n",
        "            targets_all.append(targets[si].cpu().numpy())\n",
        "\n",
        "        seen += len(images)\n",
        "\n",
        "    # Compute mAP (mean Average Precision)\n",
        "    stats = [np.concatenate(x, 0) for x in zip(*zip(*[ap_per_class(tp, conf, pred_cls, target_cls)\n",
        "                                                       for tp, conf, pred_cls, target_cls in zip(preds_all, targets_all, targets_all, targets_all)]))]\n",
        "    if stats:\n",
        "        p, r, ap, f1, ap_class = stats\n",
        "        mp, mr, map50, map = p.mean(), r.mean(), ap[:, 0].mean(), ap[:, 1].mean()\n",
        "    else:\n",
        "        mp, mr, map50, map = 0.0, 0.0, 0.0, 0.0\n",
        "\n",
        "    # Return results\n",
        "    return {\"Precision\": mp, \"Recall\": mr, \"mAP@50\": map50, \"mAP@50-95\": map}\n",
        "\n"
      ],
      "metadata": {
        "id": "zXuvWPQsFBba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = validate_yolo(\"/content/yolov5/yolov5s.pt\", dataloader, device=\"cpu\")\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "H33d7QBp5To4",
        "outputId": "1f417703-efd1-4f5d-c071-18e6b18e3903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-398-g5cdad892 Python-3.11.11 torch-2.5.1+cu124 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 2 is out of bounds for axis 0 with size 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7784b27ed054>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/yolov5/yolov5s.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-84e79c36e7d0>\u001b[0m in \u001b[0;36mvalidate_yolo\u001b[0;34m(model_path, dataloader, device, img_size, conf_thres, iou_thres)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Compute mAP (mean Average Precision)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     stats = [np.concatenate(x, 0) for x in zip(*zip(*[ap_per_class(tp, conf, pred_cls, target_cls)\n\u001b[0m\u001b[1;32m     59\u001b[0m                                                        for tp, conf, pred_cls, target_cls in zip(preds_all, targets_all, targets_all, targets_all)]))]\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-84e79c36e7d0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Compute mAP (mean Average Precision)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     stats = [np.concatenate(x, 0) for x in zip(*zip(*[ap_per_class(tp, conf, pred_cls, target_cls)\n\u001b[0m\u001b[1;32m     59\u001b[0m                                                        for tp, conf, pred_cls, target_cls in zip(preds_all, targets_all, targets_all, targets_all)]))]\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/yolov5/utils/metrics.py\u001b[0m in \u001b[0;36map_per_class\u001b[0;34m(tp, conf, pred_cls, target_cls, plot, save_dir, names, eps, prefix)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Sort by objectness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_cls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Find unique classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from yolov5.utils.metrics import ap_per_class\n",
        "from yolov5.utils.general import non_max_suppression, scale_boxes\n",
        "from yolov5.utils.torch_utils import select_device\n",
        "import time\n",
        "\n",
        "def evaluate_yolo_model(model, dataloader, device=\"cpu\", img_size=640, conf_thres=0.001, iou_thres=0.6):\n",
        "    \"\"\"\n",
        "    Evaluates a YOLOv5 model (both baseline and quantized) without reloading from a file.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The YOLOv5 model loaded in memory.\n",
        "        dataloader (torch.utils.data.DataLoader): Dataloader for the validation dataset.\n",
        "        device (str): Device to run inference ('cuda' or 'cpu').\n",
        "        img_size (int): Image size for inference.\n",
        "        conf_thres (float): Confidence threshold for detection.\n",
        "        iou_thres (float): IoU threshold for NMS.\n",
        "\n",
        "    Returns:\n",
        "        dict: Performance metrics (mAP, precision, recall, inference time).\n",
        "    \"\"\"\n",
        "    model.to(device).eval()\n",
        "    device = select_device(device)\n",
        "\n",
        "    # Metrics storage\n",
        "    stats, times = [], []\n",
        "\n",
        "    # Iterate over validation dataset\n",
        "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
        "        images = images.to(device)\n",
        "        img_shape = images.shape[2:]  # (H, W)\n",
        "\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            preds = model(images)  # Run inference\n",
        "        inference_time = time.time() - start_time\n",
        "\n",
        "        preds = non_max_suppression(preds, conf_thres, iou_thres)\n",
        "\n",
        "        for i, pred in enumerate(preds):\n",
        "            if pred is not None and len(pred):\n",
        "                # Fix: Get original shape dynamically\n",
        "                scale_boxes(img_shape, pred[:, :4], img_shape)  # Ensure shapes match\n",
        "\n",
        "            # Collect statistics\n",
        "            stats.append((pred, targets[i]))  # Store prediction & ground truth\n",
        "\n",
        "        times.append(inference_time)\n",
        "\n",
        "    # Compute metrics\n",
        "    if len(stats):\n",
        "        pred, target = zip(*stats)\n",
        "        precision, recall, ap, f1, ap_class = ap_per_class(pred, target)\n",
        "\n",
        "        results = {\n",
        "            \"mAP@50\": ap.mean().item(),\n",
        "            \"Precision\": precision.mean().item(),\n",
        "            \"Recall\": recall.mean().item(),\n",
        "            \"Avg Inference Time (s)\": sum(times) / len(times)\n",
        "        }\n",
        "    else:\n",
        "        results = {\"mAP@50\": 0, \"Precision\": 0, \"Recall\": 0, \"Avg Inference Time (s)\": 0}\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "metrics = evaluate_yolo_model(model, dataloader)\n",
        "print(metrics)\n"
      ],
      "metadata": {
        "id": "pZDjupji7MaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = validate_yolo(quantized_model, dataloader, device=\"cpu\")\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "YfjYdZzS6Z7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Zi8cs-I7eKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H1luFtyo7eHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "# Load the AIMET quantized .pth model\n",
        "quantized_model_path = \"/content/quantized.pth\"\n",
        "model = torch.load(quantized_model_path, map_location=torch.device(\"cpu\"))\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Convert to YOLOv5-compatible dictionary format\n",
        "checkpoint = {\"model\": model}  # Add other keys if necessary\n",
        "\n",
        "# Save as a new .pt file\n",
        "output_path = \"/content/quantized_yolov5_fixed.pt\"\n",
        "torch.save(checkpoint, output_path)\n",
        "\n",
        "print(f\"Fixed quantized model saved at {output_path}\")\n"
      ],
      "metadata": {
        "id": "8HAT8zP_BYtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate YOLOv5s on COCO val\n",
        "!python val.py --weights /content/quantized_yolov5_fixed.pt --data coco.yaml --img 640 --half"
      ],
      "metadata": {
        "id": "7EDTuayNAnJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def bbox_iou(box1, box2, eps=1e-6):\n",
        "    \"\"\"\n",
        "    Compute IoU (Intersection over Union) between two sets of bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        box1 (Tensor[N, 4]): First set of bounding boxes (x, y, w, h) format.\n",
        "        box2 (Tensor[M, 4]): Second set of bounding boxes (x, y, w, h) format.\n",
        "        eps (float): Small value to prevent division by zero.\n",
        "\n",
        "    Returns:\n",
        "        Tensor[N, M]: IoU values for each pair of boxes.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert [x, y, w, h] → [x1, y1, x2, y2]\n",
        "    box1_x1, box1_y1 = box1[:, 0] - box1[:, 2] / 2, box1[:, 1] - box1[:, 3] / 2\n",
        "    box1_x2, box1_y2 = box1[:, 0] + box1[:, 2] / 2, box1[:, 1] + box1[:, 3] / 2\n",
        "\n",
        "    box2_x1, box2_y1 = box2[:, 0] - box2[:, 2] / 2, box2[:, 1] - box2[:, 3] / 2\n",
        "    box2_x2, box2_y2 = box2[:, 0] + box2[:, 2] / 2, box2[:, 1] + box2[:, 3] / 2\n",
        "\n",
        "    # Compute intersection\n",
        "    inter_x1 = torch.max(box1_x1[:, None], box2_x1[None, :])\n",
        "    inter_y1 = torch.max(box1_y1[:, None], box2_y1[None, :])\n",
        "    inter_x2 = torch.min(box1_x2[:, None], box2_x2[None, :])\n",
        "    inter_y2 = torch.min(box1_y2[:, None], box2_y2[None, :])\n",
        "\n",
        "    inter_w = (inter_x2 - inter_x1).clamp(0)\n",
        "    inter_h = (inter_y2 - inter_y1).clamp(0)\n",
        "    inter_area = inter_w * inter_h\n",
        "\n",
        "    # Compute union\n",
        "    box1_area = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)\n",
        "    box2_area = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)\n",
        "    union_area = box1_area[:, None] + box2_area[None, :] - inter_area\n",
        "\n",
        "    return inter_area / (union_area + eps)\n"
      ],
      "metadata": {
        "id": "ZiKPmUep4Bk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from utils.metrics import ap_per_class\n",
        "from utils.general import non_max_suppression\n",
        "\n",
        "def validate_and_compute_metrics(dataloader, model, device=\"cuda\", conf_thresh=0.25, iou_thresh=0.5):\n",
        "    \"\"\"\n",
        "    Run inference on a DataLoader and compute mAP, Precision, Recall, and F1 Score.\n",
        "\n",
        "    Args:\n",
        "        dataloader (torch.utils.data.DataLoader): The validation dataloader.\n",
        "        model (torch.nn.Module): The trained YOLO model.\n",
        "        device (str): 'cuda' or 'cpu'\n",
        "        conf_thresh (float): Confidence threshold for predictions.\n",
        "        iou_thresh (float): IoU threshold for NMS.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing computed metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "    all_tp, all_conf, all_pred_cls, all_target_cls = [], [], [], []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradients for inference\n",
        "        for batch in dataloader:\n",
        "            images, targets = batch  # Unpack batch\n",
        "\n",
        "            # Ensure images are correctly formatted\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass to get predictions\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Apply Non-Maximum Suppression (NMS) to remove duplicate detections\n",
        "            predictions = non_max_suppression(outputs, conf_thresh, iou_thresh)\n",
        "\n",
        "            for i, preds in enumerate(predictions):  # Iterate over batch\n",
        "                target_boxes = targets[i][:, 1:] if len(targets[i]) > 0 else torch.zeros((0, 4), device=device)\n",
        "                target_labels = targets[i][:, 0] if len(targets[i]) > 0 else torch.zeros((0,), device=device)\n",
        "\n",
        "                # Ensure correct dimensions\n",
        "                if preds is None or len(preds) == 0:\n",
        "                    continue  # Skip if no valid predictions\n",
        "\n",
        "                pred_boxes = preds[:, :4]  # [x, y, w, h]\n",
        "                pred_conf = preds[:, 4]    # Objectness scores\n",
        "                pred_labels = preds[:, 5].long()  # Class predictions\n",
        "\n",
        "                # Compute IoU-based matches\n",
        "                tp = torch.zeros(pred_labels.shape[0], device=device)\n",
        "                for j, gt_box in enumerate(target_boxes):\n",
        "                    ious = bbox_iou(pred_boxes, gt_box.unsqueeze(0))  # IoU with all predictions\n",
        "                    best_iou_idx = ious.argmax()\n",
        "\n",
        "                    if ious[best_iou_idx] > iou_thresh and pred_labels[best_iou_idx] == target_labels[j]:\n",
        "                        tp[best_iou_idx] = 1  # Mark as True Positive\n",
        "\n",
        "                # Store results\n",
        "                all_tp.append(tp.cpu().numpy().flatten())\n",
        "                all_conf.append(pred_conf.cpu().numpy().flatten())\n",
        "                all_pred_cls.append(pred_labels.cpu().numpy().flatten())\n",
        "                all_target_cls.append(target_labels.cpu().numpy().flatten())\n",
        "\n",
        "    print(\"✅ DataLoader validation complete!\\n\")\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_tp = np.concatenate(all_tp) if all_tp else np.array([])\n",
        "    all_conf = np.concatenate(all_conf) if all_conf else np.array([])\n",
        "    all_pred_cls = np.concatenate(all_pred_cls) if all_pred_cls else np.array([])\n",
        "    all_target_cls = np.concatenate(all_target_cls) if all_target_cls else np.array([])\n",
        "\n",
        "    if len(all_tp) == 0:\n",
        "        print(\"⚠️ Warning: No valid targets or predictions found!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Compute mAP and other metrics\n",
        "    precision, recall, mAP50, mAP, F1, ap_class = ap_per_class(\n",
        "        all_tp, all_conf, all_pred_cls, all_target_cls, iou_thresholds=0.5\n",
        "    )\n",
        "\n",
        "    # Store results in DataFrame\n",
        "    df_metrics = pd.DataFrame({\n",
        "        \"Class\": ap_class,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"mAP@0.5\": mAP50,\n",
        "        \"mAP\": mAP,\n",
        "        \"F1 Score\": F1\n",
        "    })\n",
        "\n",
        "    return df_metrics\n",
        "\n",
        "# Example Usage\n",
        "# df_metrics = validate_and_compute_metrics(dataloader, model, device=\"cuda\")\n",
        "# print(df_metrics)\n"
      ],
      "metadata": {
        "id": "i73ANTfqZLix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics = validate_and_compute_metrics(dataloader, model, device=\"cpu\")\n",
        "print(df_metrics)"
      ],
      "metadata": {
        "id": "jPa-yPE6aoms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics = validate_and_compute_metrics(dataloader,quantized_model, device=\"cpu\")\n",
        "print(df_metrics)"
      ],
      "metadata": {
        "id": "JnJyHkJKfrSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xZ5Jw-xEfrOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fQGdpO3NfrLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ctAO45qSfrI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V2Vp1eDefrGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O8DM1u_PfrEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7jKSpV62frBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class CustomDataset(Dataset):\n",
        "#     def __init__(self, img_dir, label_dir, img_size=640, transform=None):\n",
        "#         \"\"\"\n",
        "#         Custom dataset for YOLOv5.\n",
        "\n",
        "#         Args:\n",
        "#             img_dir (str): Path to the images directory.\n",
        "#             label_dir (str): Path to the labels directory (YOLO format).\n",
        "#             img_size (int): Target image size for resizing.\n",
        "#             transform (callable, optional): Optional transform to be applied on images.\n",
        "#         \"\"\"\n",
        "#         self.img_dir = Path(img_dir)\n",
        "#         self.label_dir = Path(label_dir)\n",
        "#         self.img_size = img_size\n",
        "#         self.transform = transform\n",
        "#         self.img_files = sorted([f for f in self.img_dir.glob(\"*.jpg\")])\n",
        "#         self.label_files = sorted([self.label_dir / (f.stem + \".txt\") for f in self.img_files])\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.img_files)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_path = self.img_files[idx]\n",
        "#         label_path = self.label_files[idx]\n",
        "\n",
        "#         # Load image\n",
        "#         img = cv2.imread(str(img_path))\n",
        "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "#         img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "#         img = img / 255.0  # Normalize to [0,1]\n",
        "#         img = np.transpose(img, (2, 0, 1))  # HWC to CHW\n",
        "#         img_tensor = torch.tensor(img, dtype=torch.float32)\n",
        "\n",
        "#         # Load labels\n",
        "#         labels = []\n",
        "#         if label_path.exists():\n",
        "#             with open(label_path, \"r\") as f:\n",
        "#                 for line in f.readlines():\n",
        "#                     label = [float(x) for x in line.strip().split()]\n",
        "#                     labels.append(label)\n",
        "#         labels_tensor = torch.tensor(labels, dtype=torch.float32) if labels else torch.zeros((0, 5))\n",
        "\n",
        "#         return img_tensor, labels_tensor\n",
        "\n",
        "# Example usage:\n",
        "# dataset = CustomDataset(\"/path/to/images\", \"/path/to/labels\")\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "\n",
        "# class CustomImageDataset(Dataset):\n",
        "#     def __init__(self, img_dir, transform=None):\n",
        "#         self.img_dir = img_dir\n",
        "#         self.transform = transform\n",
        "#         self.img_paths = sorted(os.listdir(img_dir))  # Get all image paths\n",
        "#         self.labels = {}  # A dictionary for image labels (modify based on dataset)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.img_paths)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_path = os.path.join(self.img_dir, self.img_paths[idx])\n",
        "#         image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "#         if self.transform:\n",
        "#             image = self.transform(image)\n",
        "\n",
        "#         target = self.labels.get(img_path, torch.tensor([]))  # Dummy label if not found\n",
        "\n",
        "#         return image, target, img_path  # Ensure it returns all required elements\n",
        "\n",
        "# Define transformations (optional)\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((640, 640)),\n",
        "#     transforms.ToTensor()\n",
        "# ])\n",
        "\n",
        "# Load dataset\n",
        "# dataset = CustomDataset(\"/content/datasets/coco/images/val2017/\", \"/content/datasets/coco/labels/val2017\")\n",
        "# dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4,  collate_fn=custom_collate_fn )"
      ],
      "metadata": {
        "id": "IYGz9nXqfq_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# def apply_aimet_quantization(model,  dataloader):\n",
        "#     dummy_input = torch.rand(1, 3, 640, 640)\n",
        "\n",
        "#     # Print model before BN folding\n",
        "#     print(\"\\nModel before BatchNorm folding:\")\n",
        "#     print(model)\n",
        "\n",
        "#     fold_all_batch_norms(model, input_shapes=(1, 3, 640, 640), dummy_input=dummy_input)\n",
        "\n",
        "#     # Print model after BN folding\n",
        "#     print(\"\\nModel after BatchNorm folding:\")\n",
        "#     print(model)\n",
        "\n",
        "#     quant_sim = qsim.QuantizationSimModel(\n",
        "#         model,\n",
        "#         dummy_input=dummy_input,\n",
        "#         rounding_mode= 'nearest',\n",
        "#         default_param_bw= 8,\n",
        "#         default_output_bw =  8\n",
        "#     )\n",
        "\n",
        "#     # Use real calibration data for encoding computation\n",
        "#     def forward_pass(model, dataloader):\n",
        "#         model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             for i, images in enumerate(dataloader):\n",
        "#                 print(i , images[0].shape)\n",
        "#                 if i >= 10:  # Use only first 10 batches for calibration\n",
        "\n",
        "#                     break\n",
        "#                 model(images)\n",
        "#                 # print()\n",
        "\n",
        "#     quant_sim.compute_encodings(forward_pass, dataloader)\n",
        "\n",
        "\n",
        "\n",
        "    # return quant_sim.model\n",
        "    # Step 5: Define calibration function"
      ],
      "metadata": {
        "id": "BsFJwdD6fq82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from yolov5.models.common import Concat  # Import original Concat\n",
        "\n",
        "# # Define a quantization-friendly version of Concat\n",
        "# class QuantizedConcat(nn.Module):\n",
        "#     def __init__(self, dimension):\n",
        "#         super(QuantizedConcat, self).__init__()\n",
        "#         self.dimension = dimension\n",
        "\n",
        "#     def forward(self, *inputs):\n",
        "#         return torch.cat(inputs, dim=self.dimension)\n",
        "\n",
        "# # Function to replace all Concat layers in the model\n",
        "# def replace_concat_with_quantized(model):\n",
        "#     for name, module in model.named_children():\n",
        "#         if isinstance(module, Concat):\n",
        "#             print(f\"Replacing {name} with QuantizedConcat\")\n",
        "#             setattr(model, name, QuantizedConcat(dimension=1))  # Assuming concat happens along dim=1\n",
        "#         else:\n",
        "#             replace_concat_with_quantized(module)\n",
        "\n",
        "# # Load YOLOv5 model\n",
        "# from yolov5.models.yolo import Model\n",
        "# model = Model(cfg='/content/yolov5/models/yolov5s.yaml')  # Adjust config file as needed\n",
        "\n",
        "# # Load weights correctly\n",
        "# checkpoint = torch.load(\"/content/yolov5/yolov5s.pt\", map_location=\"cpu\")\n",
        "\n",
        "# # Check the keys in the checkpoint\n",
        "# if \"model\" in checkpoint:\n",
        "#     model.load_state_dict(checkpoint[\"model\"].state_dict())  # Correct way to load weights\n",
        "# else:\n",
        "#     model.load_state_dict(checkpoint)  # If only state_dict is saved\n",
        "\n",
        "# print(\"✅ YOLOv5 model loaded successfully!\")"
      ],
      "metadata": {
        "id": "gjPrvaRnt4uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !head -n 460 models/common.py"
      ],
      "metadata": {
        "id": "L8CAgAVaLM3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dummy_input = torch.randn(1, 3, 640, 640)\n",
        "\n",
        "# Export to ONNX\n",
        "onnx_path = \"/content/yolov5s.onnx\"\n",
        "torch.onnx.export(model, dummy_input, onnx_path, opset_version=12, input_names=[\"images\"], output_names=[\"output\"])\n",
        "print(f\"✅ ONNX model saved at: {onnx_path}\")\n"
      ],
      "metadata": {
        "id": "843EkpSjqM9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def replace_concat_with_quantized(module):\n",
        "#     \"\"\"Recursively replaces Concat layers with QuantizedConcat while keeping attributes.\"\"\"\n",
        "#     for name, child in module.named_children():\n",
        "#         if isinstance(child, Concat):\n",
        "#             setattr(module, name, QuantizedConcat(child.d))  # Keep the 'd' attribute\n",
        "#         else:\n",
        "#             replace_concat_with_quantized(child)  # Recursively replace submodules\n",
        "\n",
        "# replace_concat_with_quantized(model)\n"
      ],
      "metadata": {
        "id": "FSQaJzIMOLlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from aimet_torch.v2.nn import QuantizationMixin\n",
        "# from models.common import Concat\n",
        "\n",
        "# @QuantizationMixin.implements(Concat)\n",
        "# class QuantizedConcat(QuantizationMixin, Concat):\n",
        "#     def __init__(self, dimension=1):\n",
        "#         super().__init__(dimension)  # Pass 'dimension' to the parent Concat class\n",
        "#         self.input_quantizers = torch.nn.ModuleList([None, None])  # Adjust based on the number of inputs\n",
        "#         self.output_quantizers = torch.nn.ModuleList([None])\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Quantize inputs\n",
        "#         if self.input_quantizers[0]:\n",
        "#             x = [self.input_quantizers[i](x[i]) for i in range(len(x))]\n",
        "\n",
        "#         # Run forward with quantized inputs and parameters\n",
        "#         with self._patch_quantized_parameters():\n",
        "#             ret = super().forward(x)  # Calls Concat's forward()\n",
        "\n",
        "#         # Quantize output tensors\n",
        "#         if self.output_quantizers[0]:\n",
        "#             ret = self.output_quantizers[0](ret)\n",
        "\n",
        "#         return ret\n"
      ],
      "metadata": {
        "id": "Aay_GXMkKhnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx-graphsurgeon onnx\n"
      ],
      "metadata": {
        "id": "pf1IW4bkB4GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "import onnx_graphsurgeon as gs\n",
        "import numpy as np\n",
        "\n",
        "# Load ONNX model\n",
        "onnx_model = onnx.load(onnx_path)\n",
        "graph = gs.import_onnx(onnx_model)\n",
        "\n",
        "# Find and replace Concat nodes\n",
        "for node in graph.nodes:\n",
        "    if node.op == \"Concat\":\n",
        "        print(f\"🔄 Replacing {node.name} with QuantizedConcat\")\n",
        "\n",
        "        # Create a new node (simulating quantization)\n",
        "        new_node = gs.Node(\n",
        "            op=\"QuantizedConcat\",\n",
        "            name=node.name + \"_quantized\",\n",
        "            inputs=node.inputs,\n",
        "            outputs=node.outputs,\n",
        "            attrs={\"axis\": node.attrs[\"axis\"]}\n",
        "        )\n",
        "\n",
        "        # Replace in graph\n",
        "        graph.nodes.append(new_node)\n",
        "        graph.nodes.remove(node)\n",
        "\n",
        "# Save modified ONNX model\n",
        "onnx_modified_path = \"/content/yolov5s_quantized.onnx\"\n",
        "onnx.save(gs.export_onnx(graph), onnx_modified_path)\n",
        "print(f\"✅ Modified ONNX model saved at: {onnx_modified_path}\")\n"
      ],
      "metadata": {
        "id": "q2x9dUnz_9aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "pjOcI1lBCG-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "onnx_path = \"/content/yolov5s_quantized.onnx\"\n",
        "onnx_quantized_path = \"/content/yolov5s_quantized_final.onnx\"\n",
        "\n",
        "quantize_dynamic(\n",
        "    model_input=onnx_path,\n",
        "    model_output=onnx_quantized_path,\n",
        "    weight_type=QuantType.QInt8  # Uses 8-bit integer quantization\n",
        ")\n",
        "\n",
        "print(f\"✅ Quantized ONNX model saved at: {onnx_quantized_path}\")\n"
      ],
      "metadata": {
        "id": "zOkcRjsXqM6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model_reconstructed.pt\")\n"
      ],
      "metadata": {
        "id": "SHRSNl1JqM3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = apply_aimet_quantization(model,  dataloader)"
      ],
      "metadata": {
        "id": "PWyqPhyTqM1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AQdS1UsnqMy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2VXXXu74w5"
      },
      "source": [
        "# 3. Train\n",
        "\n",
        "<p align=\"\"><a href=\"https://ultralytics.com/hub\"><img width=\"1000\" src=\"https://github.com/ultralytics/assets/raw/main/im/integrations-loop.png\"/></a></p>\n",
        "Close the active learning loop by sampling images from your inference conditions with the `roboflow` pip package\n",
        "<br><br>\n",
        "\n",
        "Train a YOLOv5s model on the [COCO128](https://www.kaggle.com/datasets/ultralytics/coco128) dataset with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`.\n",
        "\n",
        "- **Pretrained [Models](https://github.com/ultralytics/yolov5/tree/master/models)** are downloaded\n",
        "automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases)\n",
        "- **[Datasets](https://github.com/ultralytics/yolov5/tree/master/data)** available for autodownload include: [COCO](https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml), [COCO128](https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml), [VOC](https://github.com/ultralytics/yolov5/blob/master/data/VOC.yaml), [Argoverse](https://github.com/ultralytics/yolov5/blob/master/data/Argoverse.yaml), [VisDrone](https://github.com/ultralytics/yolov5/blob/master/data/VisDrone.yaml), [GlobalWheat](https://github.com/ultralytics/yolov5/blob/master/data/GlobalWheat2020.yaml), [xView](https://github.com/ultralytics/yolov5/blob/master/data/xView.yaml), [Objects365](https://github.com/ultralytics/yolov5/blob/master/data/Objects365.yaml), [SKU-110K](https://github.com/ultralytics/yolov5/blob/master/data/SKU-110K.yaml).\n",
        "- **Training Results** are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n",
        "<br>\n",
        "\n",
        "A **Mosaic Dataloader** is used for training which combines 4 images into 1 mosaic.\n",
        "\n",
        "## Label a dataset on Roboflow (optional)\n",
        "\n",
        "[Roboflow](https://roboflow.com/?ref=ultralytics) enables you to easily **organize, label, and prepare** a high quality dataset with your own custom data. Roboflow also makes it easy to establish an active learning pipeline, collaborate with your team on dataset improvement, and integrate directly into your model building workflow with the `roboflow` pip package."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from yolov5.models.common import Concat  # Import original Concat\n",
        "\n",
        "# # Define a quantization-friendly version of Concat\n",
        "# class QuantizedConcat(nn.Module):\n",
        "#     def __init__(self, dimension):\n",
        "#         super(QuantizedConcat, self).__init__()\n",
        "#         self.dimension = dimension\n",
        "\n",
        "#     def forward(self, *inputs):\n",
        "#         return torch.cat(inputs, dim=self.dimension)\n",
        "\n",
        "# # Function to replace all Concat layers in the model\n",
        "# def replace_concat_with_quantized(model):\n",
        "#     for name, module in model.named_children():\n",
        "#         if isinstance(module, Concat):\n",
        "#             print(f\"Replacing {name} with QuantizedConcat\")\n",
        "#             setattr(model, name, QuantizedConcat(dimension=1))  # Assuming concat happens along dim=1\n",
        "#         else:\n",
        "#             replace_concat_with_quantized(module)\n",
        "\n",
        "# # # Load YOLOv5 model\n",
        "# # from yolov5.models.yolo import Model\n",
        "# # model = Model(cfg='/content/yolov5/models/yolov5s.yaml')  # Adjust config file as needed\n",
        "\n",
        "# # # Load weights correctly\n",
        "# # checkpoint = torch.load(\"/content/yolov5/yolov5s.pt\", map_location=\"cpu\")\n",
        "\n",
        "# # # Check the keys in the checkpoint\n",
        "# # if \"model\" in checkpoint:\n",
        "# #     model.load_state_dict(checkpoint[\"model\"].state_dict())  # Correct way to load weights\n",
        "# # else:\n",
        "# #     model.load_state_dict(checkpoint)  # If only state_dict is saved\n",
        "\n",
        "# # print(\"✅ YOLOv5 model loaded successfully!\")"
      ],
      "metadata": {
        "id": "3zK3p45_E1Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select YOLOv5 🚀 logger {run: 'auto'}\n",
        "logger = 'Comet' #@param ['Comet', 'ClearML', 'TensorBoard']\n",
        "\n",
        "if logger == 'Comet':\n",
        "  %pip install -q comet_ml\n",
        "  import comet_ml; comet_ml.init()\n",
        "elif logger == 'ClearML':\n",
        "  %pip install -q clearml\n",
        "  import clearml; clearml.browser_login()\n",
        "elif logger == 'TensorBoard':\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir runs/train"
      ],
      "metadata": {
        "id": "i3oKtE4g-aNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcFxRcFdJ_O"
      },
      "source": [
        "# Train YOLOv5s on COCO128 for 3 epochs\n",
        "!python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15glLzbQx5u0"
      },
      "source": [
        "# 4. Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comet Logging and Visualization 🌟 NEW\n",
        "\n",
        "[Comet](https://www.comet.com/site/lp/yolov5-with-comet/?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=yolov5_colab) is now fully integrated with YOLOv5. Track and visualize model metrics in real time, save your hyperparameters, datasets, and model checkpoints, and visualize your model predictions with [Comet Custom Panels](https://www.comet.com/docs/v2/guides/comet-dashboard/code-panels/about-panels/?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=yolov5_colab)! Comet makes sure you never lose track of your work and makes it easy to share results and collaborate across teams of all sizes!\n",
        "\n",
        "Getting started is easy:\n",
        "```shell\n",
        "pip install comet_ml  # 1. install\n",
        "export COMET_API_KEY=<Your API Key>  # 2. paste API key\n",
        "python train.py --img 640 --epochs 3 --data coco128.yaml --weights yolov5s.pt  # 3. train\n",
        "```\n",
        "To learn more about all of the supported Comet features for this integration, check out the [Comet Tutorial](https://docs.ultralytics.com/yolov5/tutorials/comet_logging_integration). If you'd like to learn more about Comet, head over to our [documentation](https://www.comet.com/docs/v2/?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=yolov5_colab). Get started by trying out the Comet Colab Notebook:\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RG0WOQyxlDlo5Km8GogJpIEJlg_5lyYO?usp=sharing)\n",
        "\n",
        "<a href=\"https://bit.ly/yolov5-readme-comet2\">\n",
        "<img alt=\"Comet Dashboard\" src=\"https://user-images.githubusercontent.com/26833433/202851203-164e94e1-2238-46dd-91f8-de020e9d6b41.png\" width=\"1280\"/></a>"
      ],
      "metadata": {
        "id": "nWOsI5wJR1o3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ClearML Logging and Automation 🌟 NEW\n",
        "\n",
        "[ClearML](https://cutt.ly/yolov5-notebook-clearml) is completely integrated into YOLOv5 to track your experimentation, manage dataset versions and even remotely execute training runs. To enable ClearML (check cells above):\n",
        "\n",
        "- `pip install clearml`\n",
        "- run `clearml-init` to connect to a ClearML server (**deploy your own [open-source server](https://github.com/allegroai/clearml-server)**, or use our [free hosted server](https://cutt.ly/yolov5-notebook-clearml))\n",
        "\n",
        "You'll get all the great expected features from an experiment manager: live updates, model upload, experiment comparison etc. but ClearML also tracks uncommitted changes and installed packages for example. Thanks to that ClearML Tasks (which is what we call experiments) are also reproducible on different machines! With only 1 extra line, we can schedule a YOLOv5 training task on a queue to be executed by any number of ClearML Agents (workers).\n",
        "\n",
        "You can use ClearML Data to version your dataset and then pass it to YOLOv5 simply using its unique ID. This will help you keep track of your data without adding extra hassle. Explore the [ClearML Tutorial](https://docs.ultralytics.com/yolov5/tutorials/clearml_logging_integration) for details!\n",
        "\n",
        "<a href=\"https://cutt.ly/yolov5-notebook-clearml\">\n",
        "<img alt=\"ClearML Experiment Management UI\" src=\"https://github.com/thepycoder/clearml_screenshots/raw/main/scalars.jpg\" width=\"1280\"/></a>"
      ],
      "metadata": {
        "id": "Lay2WsTjNJzP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPvRbS5Swl6"
      },
      "source": [
        "## Local Logging\n",
        "\n",
        "Training results are automatically logged with [Tensorboard](https://www.tensorflow.org/tensorboard) and [CSV](https://github.com/ultralytics/yolov5/pull/4148) loggers to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc.\n",
        "\n",
        "This directory contains train and val statistics, mosaics, labels, predictions and augmentated mosaics, as well as metrics and charts including precision-recall (PR) curves and confusion matrices.\n",
        "\n",
        "<img alt=\"Local logging results\" src=\"https://user-images.githubusercontent.com/26833433/183222430-e1abd1b7-782c-4cde-b04d-ad52926bf818.jpg\" width=\"1280\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zelyeqbyt3GD"
      },
      "source": [
        "# Environments\n",
        "\n",
        "YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n",
        "\n",
        "- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
        "- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n",
        "- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n",
        "- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qu7Iesl0p54"
      },
      "source": [
        "# Status\n",
        "\n",
        "![YOLOv5 CI](https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg)\n",
        "\n",
        "If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEijrePND_2I"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "Additional content below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMusP4OAxFu6"
      },
      "source": [
        "# YOLOv5 PyTorch HUB Inference (DetectionModels only)\n",
        "import torch\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True, trust_repo=True)  # or yolov5n - yolov5x6 or custom\n",
        "im = 'https://ultralytics.com/images/zidane.jpg'  # file, Path, PIL.Image, OpenCV, nparray, list\n",
        "results = model(im)  # inference\n",
        "results.print()  # or .show(), .save(), .crop(), .pandas(), etc."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}